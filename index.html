<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Heyosam</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="Heyosam">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Heyosam">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Bench Lian">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Heyosam" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Heyosam</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-Neo一个学习型查询优化器" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/03/17/Neo%E4%B8%80%E4%B8%AA%E5%AD%A6%E4%B9%A0%E5%9E%8B%E6%9F%A5%E8%AF%A2%E4%BC%98%E5%8C%96%E5%99%A8/" class="article-date">
  <time class="dt-published" datetime="2021-03-17T03:41:47.263Z" itemprop="datePublished">2021-03-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="Neo-一个学习型查询优化器"><a href="#Neo-一个学习型查询优化器" class="headerlink" title="Neo:一个学习型查询优化器"></a>Neo:一个学习型查询优化器</h1><blockquote>
<p>Ryan Marcus, Parimarjan Negi, Hongzi Mao, Chi Zhang,Mohammad Alizadeh, Tim Kraska, Olga Papaemmanouil, Nesime Tatbul</p>
<p>Brandeis University,MIT,Intel Labs</p>
</blockquote>
<h2 id="ABSTRACT"><a href="#ABSTRACT" class="headerlink" title="ABSTRACT"></a>ABSTRACT</h2><p>​        查询优化是数据库系统中最具挑战性的问题之一。尽管在过去几十年取得了进展，但查询优化器仍然是非常复杂的组件，需要对特定的工作负载和数据集进行大量手动调优。受到将机器学习应用于数据管理挑战的最新进展的启发，我们介绍Neo(神经网络优化器)，一个基于学习的新型查询优化器，它依赖于深度神经网络来生成查询执行计划。Neo从现有的优化器中引导其查询优化模型，并继续从收益查询中学习，在成功的基础上再接再厉，并从失败中学习。此外，Neo自然适应底层数据模式，对估计误差具有鲁棒性。实验结果表明，Neo即使从PostgreSQL这样的简单优化器中引导，也可以学习到提供与最先进的商业优化器类似性能的模型，在某些情况下甚至超过它们。</p>
<h2 id="1-INTRODUCTION"><a href="#1-INTRODUCTION" class="headerlink" title="1.INTRODUCTION"></a>1.INTRODUCTION</h2><p>面对层出不穷的机器学习成功案例，每个数据库研究者可能都想过是否可以学习一个查询优化器。查询优化是在数据库系统中获得良好性能的关键，并且可以将查询的执行时间加快几个数量级。然而，目前构建一个好的优化器需要成千上万的人工时间，这是一门只有少数专家完全掌握的艺术。更糟糕的是，查询优化器需要繁琐地维护，尤其是随着系统的执行和存储引擎的发展。因此，没有一个免费的开源查询优化器的性能能够与IBM、Oracle或微软提供的商业优化器相媲美。</p>
<p>由于查询优化基于启发式的本质，在过去的几十年里，已经有很多人尝试通过学习来改进查询优化器。例如，近二十年前，DB2提出的学习型优化器Leo[54]。Leo通过随着时间的推移调整其基数估计，从错误中学习。然而，Leo仍然需要一个人工设计的成本模型，一个精心挑选的搜索策略，以及大量的开发者调整的启发式方法，这些都需要多年的时间来开发和完善。此外，Leo只能学习到更好的基数估计，而不能学习到新的优化策略（例如，如何考虑基数估计的不确定性，操作符选择等）。</p>
<p>最近，数据库社区已经开始探索如何使用神经网络来提高查询优化器[35,60]。例如，DQ[22]和ReJOIN[34]使用强化学习与人工成本模型相结合来自动学习搜索策略，以探索可能的连接顺序空间。<br>虽然这些论文表明，学习到的搜索策略可以在提供的成本模型上优于传统的启发式算法，但它们并没有显示出对实际查询性能的一致性或显著影响。此外，它们仍然依赖于优化器的启发式算法进行基数估计、运算符选择和候选执行计划的成本估计。</p>
<p>其他一些方法演示了如何使用机器学习来实现更好的基数估计[20,43,44]。然而，没有一个证明它们改进的基数估计实际上会带来更好的查询计划。改进基数估计的平均误差非常容易，但是对于实际改进查询计划[26]的情况，改进估计就困难得多了。此外，基数估计只是优化器的一个组成部分。与连接顺序选择不同，识别连接操作符(例如，哈希连接、归并连接)和选择索引不能(完全)简化为基数估计。最后，SkinnerDB表明自适应查询处理策略可以从强化学习中受益，但需要一个特殊化的查询执行引擎，而不能从运算符流水线或其他高级并行模型[56]中受益。</p>
<p>换句话说，最近基于机器学习的所有方法接近于学习整个优化器，没有一个展示了他们的技术是否达到最先进的性能（据我们所知，这些技术都没有达到最先进的性能）。但是整个优化器可以被学习，这仍然是一个重要的里程碑，具有深远的意义。最重要的是，如果一个学习型的查询优化器能够在短时间内达到与商业系统相当的性能，那么创建一个新的查询优化器所需的人力开发时间就会大大减少。反过来，这将使好的优化器可用于更广泛的系统，并可改善目前使用开源数据库的数千个应用程序的性能。此外，它可以改变查询优化器的构建方式，用更整体的优化问题取代昂贵的启发式方法。这可能会带来更好的可维护性，以及使得优化器真正从它们的错误中吸取教训并调整它们对某一特定客户实例的整个策略，以 实现实例优化[21]。</p>
<p>在这片文章中，我们向构建一个具有最好性能的端到端的学习型优化器迈出标志性的一步。据我们所知，这是第一个表明可以学习整个查询优化器的工作。我们的优化器能够达到与最先进的商业优化器(如Oracle和Microsoft)相似的性能，有时甚至超过它们。这需要克服一些关键挑战,从捕获查询语义向量,处理基于树的查询计划结构,设计一种搜索策略,结合物理操作符和索引选择,用神经网络代替人工成本模型,采用强化学习技术不断改进，显著缩短给定数据库的训练时间。所有这些技术都集成到第一个端到端学习查询优化器中，称为Neo(神经网络优化器)。</p>
<p>Neo学习如何决定连接顺序、物理操作符选择和索引选择。然而，我们还没有达到从头学习这些任务的里程碑。首先，Neo仍然需要关于所有可能的查询重写规则的先验知识(这保证了语义的正确性，并且规则的数量通常很少)。其次，我们将Neo限制为project-select-equijoin-aggregate-queries(尽管我们的框架是通用的，可以很容易地扩展)。第三，我们的优化器还没有从一个数据库泛化到另一个数据库，因为我们的特性是特定于一组表的——然而，Neo确实泛化到不可见的查询，它可以包含任何数量的已知表。第四，Neo需要一个传统的(较弱的)查询优化器来引导其学习过程。正如[35]中提出的，我们使用传统的查询优化器作为专家演示的来源，Neo使用它引导其初始策略。这种技术被称为“从演示中学习”[9,16,49,50]，能显著加快学习过程，将训练时间从几天/几周降低到几个小时。用于引导Neo的查询优化器在性能上可能会差很多，经过一段时间的训练后，Neo超过了性能，不再需要它。对于这项工作，我们使用PostgreSQL优化器，但是任何传统的(开源的)优化器都可以使用。</p>
<p>我们的结果表明，即使使用PostgreSQL优化器，Neo在自己的查询执行引擎上的性能也优于商业优化器。有趣的是，Neo学习自动调整基数预测精度的变化(例如，如果基数估计不太精确，它会选择更健壮的计划)。此外，它可以根据客户偏好信息进行调优(例如，是提高最差情况性能还是提高相对性能)——这是很难用传统技术实现的。</p>
<p>我们认为，Neo代表着在构建一个完全可学习的优化器方面向前迈进了一步。此外，目前的Neo已经可以用来改善数千个依赖于PostgreSQL和其他开源数据库系统(如SQLite)的应用程序的性能。我们希望Neo能激励其他数据库研究人员尝试将查询优化器结合起来，以新的方式学习系统，就像AlexNet[23]改变了图像分类器构建的方式一样。</p>
<p>综上所述，我们做出了以下贡献:</p>
<ul>
<li>我们提出了一种端到端的学习方法来优化查询，包括连接排序、索引选择和物理操作符选择。</li>
<li>我们表明，在对数据集和代表的实验性样本查询工作负载进行训练后，Neo能够生成，即使是在它以前没有遇到过的查询上。</li>
<li>我们评估了不同的特征工程技术，并提出了一种新技术，它隐式地表示数据集内的相关性。</li>
<li>我们证明，经过少量的训练，Neo能够在各自的数据库系统上达到与Oracle和Microsoft的查询优化器相当的性能。</li>
</ul>
<p>本文的其余部分组织如下:第2节概述了Neo的学习框架和系统模型。第3节描述了如何用Neo表示查询和查询计划。第4节解释Neo的价值网络，这是我们系统的核心学习组件。第5节描述了行向量嵌入，这是底层数据库的一种可选的学习表示，帮助Neo理解用户数据之间的相关性。第6节给出了Neo的实验评价，第7节讨论了相关工作，第8节给出了总结意见。</p>
<h2 id="2-LEARNING-FRAMEWORK-OVERVIEW"><a href="#2-LEARNING-FRAMEWORK-OVERVIEW" class="headerlink" title="2.LEARNING FRAMEWORK OVERVIEW"></a>2.LEARNING FRAMEWORK OVERVIEW</h2><p>Neo的独特之处在于它是第一个端到端查询优化器。如表1所示，它通过机器学习模型替换了传统的Selinger风格[52]查询优化器的每个组件:(i)查询表示是通过特性而不是基于对象的查询操作符树(例如，Volcano风格的[15]迭代器树);(ii)成本模型是一个深度神经网络(DNN)模型，而不是手工制作的方程;(iii)搜索策略是DNN引导的学习最佳优先搜索策略，而不是计划空间枚举或动态规划;(iv) 基数估计是基于直方图或学习向量嵌入方案，结合学习模型，而不是手工调整的基于直方图的基数估计模型。最后，(v) Neo使用强化学习和从演示中学习，将其集成到端到端查询优化器中，而不是依赖于人类工程。虽然我们在下表所列出的各个部分中描述了不同的组件，但下面提供了Neo学习方式的总体概述，如图1所示。</p>
<table>
<thead>
<tr>
<th align="center"></th>
<th align="center">传统优化器</th>
<th align="center">神经网络优化器(Neo)</th>
</tr>
</thead>
<tbody><tr>
<td align="center">创建</td>
<td align="center">开发人员</td>
<td align="center">强化学习(第2节)</td>
</tr>
<tr>
<td align="center">查询表示</td>
<td align="center">操作符树</td>
<td align="center">特征编码(第3节)</td>
</tr>
<tr>
<td align="center">代价模型</td>
<td align="center">人工制作模型</td>
<td align="center">学习型DNN模型(第4节)</td>
</tr>
<tr>
<td align="center">计划空间枚举</td>
<td align="center">启发式策略，动态规划</td>
<td align="center">DNN指导的搜索策略(第4.2节)</td>
</tr>
<tr>
<td align="center">基数估计</td>
<td align="center">柱 状图，人工制作模型</td>
<td align="center">柱状图，learned embeddings(第5节)</td>
</tr>
</tbody></table>
<p><img src="C:\Users\12704\Documents\blog\Neo一个学习型查询优化器.assets\image-20210316170924093.png" alt="图1"></p>
<p><strong>专验知识集合.</strong> 第一个阶段，标记为专业知识，最初从传统的查询查询生成经验，这在[35]中提出。Neo假设存在应用程序提供的示例工作负载，由代表应用程序总工作负载的查询组成。此外，我们假设Neo可以访问一个简单、传统的或基于成本的专家优化器(例如，Selinger [52]， PostgreSQL[1])。这个简单的优化器被视为一个黑盒，仅用于为示例工作负载中的每个查询创建查询执行计划(QEPs)。这些QEPs和它们的延迟被添加到Neo的经验中(例如，一组计划/延迟对)，它们将被用作下一个模型训练阶段的起点。注意，专家优化器可能与底层数据库执行引擎无关。</p>
<p>**模型建立.**根据收集到的经验，Neo建立了一个初始价值模型。价值模型是一种深度神经网络，其结构设计用于预测给定查询的给定部分或完整计划的最终执行时间。我们以监督的方式使用收集的经验训练价值网络。这个过程涉及到将每个用户提交的查询转换(通过feataturizer模块)形成对机器学习模型有用的特性。这些特性既包含查询级信息(例如，连接图、谓词属性等)，也包含计划级信息(例如，选择的连接顺序、访问路径等)。Neo可以使用许多不同的特征化技术，从简单的单热编码(第3.2节)到更复杂的embeddings(第5节)。Neo的价值网络使用树卷积[40]来处理树形结构的QEPs(第4.1节)。</p>
<p>**计划搜索.**一旦查询级别的信息被编码，Neo就使用值模型在QEPs空间(即选择连接顺序、连接操作符和索引)进行搜索，并发现具有最小预期执行时间(即值)的计划。由于特定查询的所有执行计划的空间太大，无法进行详尽的搜索，Neo使用价值模型作为启发式方法，对该空间执行最佳优先搜索。Neo创建的完整计划，包括连接顺序、连接操作符(例如:散列、合并、循环)和访问路径(例如，索引扫描，全表扫描)被发送到底层执行引擎，该引擎负责应用语义有效的查询重写规则(例如，插入必要的散列和排序操作)并执行最终计划。这确保了Neo生成的每个执行计划都能计算出正确的结果。计划搜索将在4.2节中详细讨论。</p>
<p>**模型优化.**随着新的查询通过Neo进行优化，模型被迭代改进和定制，以适应底层数据库和执行引擎。这是通过整合新收集的关于QEP和性能的经验来实现的。具体来说，一旦为特定查询选择了QEP，就将其发送到底层执行引擎，该引擎处理查询并将结果返回给用户。此外，Neo记录QEP最终执行延迟，将计划/延迟对添加到其经验中。然后，Neo根据这种经验重新训练价值模型，反复改进其评估。</p>
<p>**讨论.**这个过程——特征化、搜索和细化——对于用户发送的每个查询都要重复。Neo的架构设计是为了建立一个纠正性的反馈循环：当Neo的学习成本模型引导Neo预测到一个性能良好的查询计划，但结果延迟很高时，Neo的成本模型会学习预测性能差的计划的成本更高。因此，Neo在未来选择与表现不佳的计划属性相似的计划的可能性较小。因此，Neo的成本模型变得更加准确，有效地从错误中学习。</p>
<p>Neo的架构，使用一个学习成本模型来引导搜索通过一个大而复杂的空间，灵感来自于AlphaGo[53]，一个为玩围棋而开发的强化学习系统。在较高水平的围棋比赛中，AlphaGo使用神经网络来评估每一步棋的可取性，并使用搜索程序来寻找最有可能让你获胜的一系列棋。<br>类似地，Neo使用神经网络来评估部分查询计划的可取性，并使用搜索函数来查找可能导致较低延迟的完整查询计划。</p>
<p>AlphaGo和Neo都从专家那里获得了成本模型。AlphaGo从人类专家玩的围棋游戏数据集出发，Neo从传统查询优化器(由人类专家设计)构建的查询执行计划数据集出发。这个引导的原因是由于加强学习固有的样本低效[16,49]:如果不带引导，像Neo或AlphaGo这样的强化学习算法可能需要数百万次迭代[38]，甚至成为与人类专家竞争。引导来源于专家(即，从示范学习)已被证明大大减少了学习一个好策略所需的迭代次数[16,50]。减少样例的低效率对于数据库管理系统来说尤其关键:每次迭代都需要执行一个查询，而用户在达到与当前查询优化器同等的性能之前，不太可能愿意执行数百万个查询。更糟糕的是，执行一个糟糕的查询执行计划所花费的时间比执行一个好的执行计划所花费的时间要长，因此初始迭代将花费不可行的时间来完成[35]。</p>
<p>因此，Neo可以被看作是一个类似于AlphaGo的从演示中学习的强化学习系统——但是，AlphaGo和Neo有很多不同之处。首先，由于围棋板的网格性质，AphaGo可以将棋盘简单地表示为一幅图像，并使用图卷积(可能是研究最充分、高度优化的神经网络原语[24,28])来预测棋盘状态的理想性。另一方面，查询执行计划具有树形结构，不能简单地表示为图像，图卷积解决方案也不容易应用。第二，在围棋中，棋盘代表了与某一特定动作相关的所有信息，而且可以用不到一千字节的存储空间来表示。在查询优化中，用户数据库中的数据与查询执行计划的性能是高度相关的，并且通常比千字节大得多(不可能简单地将用户的整个数据库提供给神经网络)。第三，AlphaGo只有一个明确的目标:击败对手，达到获胜状态。另一方面，Neo需要考虑用户的偏好，例如:Neo应该优化平均情况还是最坏情况延迟?</p>
<p>本文的其余部分将详细描述我们针对这些问题的解决方案，首先介绍查询计划的符号和编码。</p>
<h2 id="3-QUERY-FEATURIZATION"><a href="#3-QUERY-FEATURIZATION" class="headerlink" title="3. QUERY FEATURIZATION"></a>3. QUERY FEATURIZATION</h2><p>在本节中，我们将描述如何将查询计划表示为向量，首先使用一些必要的符号。</p>
<h3 id="3-1-Notation"><a href="#3-1-Notation" class="headerlink" title="3.1 Notation"></a>3.1 Notation</h3>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2021/03/17/Neo%E4%B8%80%E4%B8%AA%E5%AD%A6%E4%B9%A0%E5%9E%8B%E6%9F%A5%E8%AF%A2%E4%BC%98%E5%8C%96%E5%99%A8/" data-id="ckmcwjjuc0000o0sqcje52o92" data-title="" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-PAlab-report" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/03/17/PAlab-report/" class="article-date">
  <time class="dt-published" datetime="2021-03-17T03:36:40.007Z" itemprop="datePublished">2021-03-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/03/17/PAlab-report/">PA</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="系统能力综合培养课程报告"><a href="#系统能力综合培养课程报告" class="headerlink" title="系统能力综合培养课程报告"></a>系统能力综合培养课程报告</h1><p>CS1705班 U201714707 练炳诚</p>
<hr>
<h2 id="我完成的内容"><a href="#我完成的内容" class="headerlink" title="我完成的内容"></a>我完成的内容</h2><p>PA1</p>
<p>PA2</p>
<p>PA3</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2021/03/17/PAlab-report/" data-id="ckmcwaa5q0000rksq1agbbma1" data-title="PA" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-AlphaJoin 连接顺序选择器" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/03/17/AlphaJoin%20%E8%BF%9E%E6%8E%A5%E9%A1%BA%E5%BA%8F%E9%80%89%E6%8B%A9%E5%99%A8/" class="article-date">
  <time class="dt-published" datetime="2021-03-17T03:36:40.007Z" itemprop="datePublished">2021-03-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/03/17/AlphaJoin%20%E8%BF%9E%E6%8E%A5%E9%A1%BA%E5%BA%8F%E9%80%89%E6%8B%A9%E5%99%A8/">AlphaJoin</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="AlphaJoin-连接顺序选择器"><a href="#AlphaJoin-连接顺序选择器" class="headerlink" title="AlphaJoin: 连接顺序选择器"></a>AlphaJoin: 连接顺序选择器</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>查询优化至今仍然是一个困难的问题，现存的DBMS经常错过一个好的连接顺序，识别一个好的连接顺序是提高数据库性能的关键，主要的挑战在于连接顺序的选择需要枚举的可能连接顺序，在一个较大的搜索空间内能提升找到更优解的可能性，但也会提升查询的时间开销。</p>
<p>受到AlphaGo的启发，提出一个叫<strong>AlphaJoin</strong>的方法，应用了AlphaGo中的<strong>蒙特卡洛树搜索</strong>，目前的研究表明该方法在性能上完全超过了PostgreSQL采用的最先进的方法。</p>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>数据库调优在提高DBMS性能上是很重要的一点，传统的启发式方法如动态规划，贪心算法，遗传算法和模拟退火算法等依赖于全局的静态信息，它们工作的时候没有学习历史的查询信息。面对最近ML在解决各种计算科学问题上的成功，很自然地就会想到用ML的方法来为表连接顺序选择进行优化。Marcus等人提出一个待验证阶段的选择顺序产生器ReJOIN，使用了强化学习的方法。目前的研究结果表明ReJOIN比PostgreSQL在产生选择顺序上做得更好，然而它跟传统优化器一样是依赖于<strong>代价模型</strong>的，代码模型是人为设计的，依赖于静态的信息，不会动态的改变。换言之，有可能选择出的方案代价最小，但真实执行时间并不是最小。Marcus等人提出一个名叫NEO的学习模型来生成高效的连接顺序，它使用真实执行时间而非代价模型，可以达到甚至超过传统优化器的优化效果。然而，这些方法都是基于简单搜索策略，这种策略搜索<strong>不均衡</strong>，有可能陷入局部最优解。</p>
<p>受到AlphaGo的启发，我们提出了采用蒙特卡洛树的搜索方案，主要思想是模拟许多连接顺序的可能在一个树结构上来进行搜索，具体来说，我们做了以下几点工作：</p>
<ol>
<li>第一次将MCTS用于学习和生成高效的连接顺序，设计了一个神经网络(Order Value Network,OVN)来预测给定查询计划的执行时间，并且在该网络内执行MCTS为查询计划打分，这是AlphaJoin 1.0。</li>
<li>基于AlphaJoin 1.0，我们设计了一个自适应决策网络(Adaptive Decision Network,ADN)来选择最终的连接顺序是由OVN给出还是原始查询优化器给出。</li>
<li>我们的实验结果证明了AlphaJoin能够产生有效的连接顺序，相对于NEO和PostgreSQL的传统优化器来说性能更好。</li>
</ol>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>在这一节中，我们首先描述两个编码方式(查询语句编码和查询计划编码)，然后对AlphaJoin进行一个综述。</p>
<h3 id="Encodings"><a href="#Encodings" class="headerlink" title="Encodings"></a>Encodings</h3><p><img src="/images/AlphaJoin%20%E8%BF%9E%E6%8E%A5%E9%A1%BA%E5%BA%8F%E9%80%89%E6%8B%A9%E5%99%A8/image-20201217171517053.png" alt="图1"></p>
<p><strong>SQL-encoding</strong> ：对SQL查询计划中的<strong>表及属性信息</strong>进行编码，每个查询计划可表示成两个部分，第一个部分将查询的连接信息以邻接矩阵进行保存，如上图中第1行第3列为1则表示表’A’和表’C’进行连接；第二个部分对包含的SQL谓词2属性进行简单的”ope-hot encoding”，例如B.a2为1则表示对表’B’的’a2’属性列有选择条件。</p>
<p><strong>Plan-encoding</strong>：除了SQL=encoding外，我们还需要表示部分或完整的查询计划，每个编码和相应的查询计划之间是<strong>一一对应</strong>的，即计划编码是可编码和可解码的。在前人所做的工作ReJOIN中，使用了Huffman编码，这种方式难以表示稠密树，不能区分左右子树，也不能区分主动表和被动表。本文设计了一个查询计划编码，也包含两个部分（见上图）。</p>
<p>与SQL-encoding唯一不同之处在于，我们用查询顺序取代了原来的编码矩阵中的”1”，例如上图中第三行第五列为”4”，表示最先执行的表连接，即先连接表”C”和表”E”得到CE，然后第四行第三列为”3”，表示连接表”D”和表”C”，然而表”C”已经和表”E”连接过了，所以是将表”D”与连接结果CE进行连接，即D(CE)。随后，第一行第二列为”2”，表示对表”A”和表”B”进行连接得到AB。最后，第三行第一列为”1”，表示将表”C”与表”A”进行连接，然而表”C”和表”A”都连接过了，即表示将包含表”C”的连接结果与包含表”A”的连接结果进行连接，即(D(CE))(AB)。按照这个原则，解码信息包含了连接顺序与主动表和被动表的区分（行表示主动表，列表示被动表），解码出来的连接顺序是唯一的。这个编码方式类似于AlphaGO中的moves编码。</p>
<h3 id="AlphaJoin1-0"><a href="#AlphaJoin1-0" class="headerlink" title="AlphaJoin1.0"></a>AlphaJoin1.0</h3><p>AlphaJoin1.0包含两个部分：Order Value Network（顺序价值网络）和MCTS（Monte Carlo tree Search,蒙特卡洛树搜索）。</p>
<h4 id="Order-Value-Network"><a href="#Order-Value-Network" class="headerlink" title="Order Value Network"></a>Order Value Network</h4><p>Order value network（OVN）是一种用来预测部分执行计划的最佳查询执行时间的深度神经网络，其架构如下图。</p>
<p><img src="/images/AlphaJoin%20%E8%BF%9E%E6%8E%A5%E9%A1%BA%E5%BA%8F%E9%80%89%E6%8B%A9%E5%99%A8/image-20201221112542857.png" alt="图2"></p>
<p>它包含了一个输入层I，三个使用了<em>ReLU</em>作为激活函数的隐藏层H，和一个输出层O。因为这个神经网络的目标是估计一个具体的执行计划所花费的执行时间是快还是慢，所以输入数据是plan-encoding，输出是一个多标签的分类结果，从K=0到K=4，K越小则表明预测的时间越小（即用一个离散的度来衡量执行时间的大小，而不是具体的时间单位），这里K取4是实验得到的最佳取值范围。我们使用了<em>softmax</em>函数在执行时间范围内生成适当的概率分布以及<em>dropout</em>正则化来防止过拟合。</p>
<p>我们训练我们的网络来最小化历史查询计划和它们相应的预测执行时间的交叉熵（多分类问题的标准损失），我们研究了卷积神经网络，但没有发现明显的性能改善。我们的模型在Join Order Benchmark（JOB）上达到了60.9%的准确率，尽管这个结果与其他的预测任务的结果不能相比，但经过MCTS的大量仿真来选择合适的连接顺序，将弥补网络模型的准确性（AlphaGo的准确率也只达到了50%，但任然表现出了良好的性能）。</p>
<h4 id="MCTS-for-AlphaJoin"><a href="#MCTS-for-AlphaJoin" class="headerlink" title="MCTS for AlphaJoin"></a>MCTS for AlphaJoin</h4><p>这一节我们首先介绍UCT算法，MCTS中的reward function，然后讨论MCTS是如何工作的。</p>
<p><strong>UCT Algorithm：</strong>全称是”应用于树的上限置信区间算法（Upper Confidence Bounds applied to Trees, UCT ）”，这是一种博弈树搜索算法，用来解决选择树节点的问题。该算法采不仅拥有搜索模型的能力，还能挖掘更多以前从未尝试过的树节点，以减少陷入局部最优的可能性。公式如下：<br>$$<br>UCT(v_i,v)=\frac{Q(v_i)}{N(v_i)} + C\sqrt{\frac{logN(v)}{N(v_i)}}<br>$$</p>
<p>其中$v_i$是当前节点，$v$是它的父节点，$Q(v_i)$表示在当前节点获得优势的次数，$N(v_i)(N(v))$是当前节点访问的总次数，$C$是探索敏感度参数。</p>
<p>以AlphaGo为例，公式的第一部分称为exploitation，是当前节点获得优势的次数与搜索次数的比值，这决定了选择该节点后获胜的概率，第二部分称为exploration，若子节点访问次数$N(v_i)$越小则这部分的值越大，这让未被充分搜索的节点获得了更大的被搜索的机会。</p>
<p><strong>Reward Function：</strong>与AlphaGo类似，在MCTS获得一个完整的连接顺序后，我们需要定义”获胜”情况。在查询优化问题中，我们的目标是生成一个具有最小执行时间的查询计划，因此，更小的执行时间意味着更高的获胜可能。我们设计了一个简单的奖励函数如下：<br>$$<br>R_j=\frac{K-k_j}{K} ,\quad k_j=0,1,2,3,4<br>$$<br>其中$K$是前面在OVN中定义的执行时间的度，$k_j$是尝试的连接顺序$j$所预测的执行时间的度，这个函数取值为1则该计划的执行时间表明预测的执行时间的度是最小值0，即效果最好。</p>
<p><strong>MCTS for Join Order Selection：</strong> MCTS算法是一种应用蒙特卡洛方法进行树搜索的决策算法。MCTS通过模拟将树展开，在搜索空间直到做出决策，并将决策的最终结果反馈给树中的节点进行更新。经过大量的模拟后，每个节点的信息表示为正确决策的数量与该节点的模拟总数的比例。为了降低从根节点到叶子节点的搜索空间，我们将每个节点需要模拟的次数定义为$S_n=N_c*F_s$，其中$N_c$是当前节点下子节点的数量，$F_S$是控制搜索次数的因子。每个SQL查询的模拟次数是$\sum^J_1S_n$，其中$J$是本次查询的连接数。MCTS的过程会持续到搜索完所有的连接顺序或者达到最大模拟次数，每次模拟可以分为以下四个步骤：</p>
<ol>
<li>**Selection-**用UCT算法来选择从根节点到叶子节点的连接顺序，一旦在遍历过程中遇到子节点，则进入expansion步骤。</li>
<li>**Expansion-**如果选择的当前节点没有达到终止条件，则创建一个该节点的子节点.</li>
<li>**Simulation-**用随机策略向下进行选择模拟直至达到终止节点。</li>
<li>**Back Propagation-**从新节点到根节点执行反向传播过程。在这个过程中，每个节点中存储的模拟总数增加1，如果新节点的仿真结果执行时间更短，这些节点也会增加更新reward。</li>
</ol>
<p>这个过程见下图</p>
<p><img src="/images/AlphaJoin%20%E8%BF%9E%E6%8E%A5%E9%A1%BA%E5%BA%8F%E9%80%89%E6%8B%A9%E5%99%A8/MCTS%E8%BF%87%E7%A8%8B.png" alt="图3"></p>
<p><strong>Preliminary Results for AlphaJoin 1.0：</strong>我们首先探索了$F_s$对选择器性能的影响，然后通过一系列基准测试对比了AlphaJoin、PostgreSQL的查询优化器、NEO之间的性能，结果如下图</p>
<p><img src="/images/AlphaJoin%20%E8%BF%9E%E6%8E%A5%E9%A1%BA%E5%BA%8F%E9%80%89%E6%8B%A9%E5%99%A8/AlphaJoin1.0%E6%80%A7%E8%83%BD.png" alt="图4"></p>
<p>图(a)说明了不同的因子$F_s$对优化执行时间和搜索时间的影响，我们列举了$F_s$从5到25的取值，发现AlphaJoin的优化执行时间不断减小，但搜索时间逐渐增加，因为$F_s$越大MCTS过程的则模拟次数越大。因此，进行一个综合权衡后我们认为最佳的$F_s$取值是15。</p>
<p>图(b)展示了三种优化器的性能差异，说明即使我们将搜索时间也考虑在内，AlphaJoin的性能任然高于另外两者。实验表明MCTS能有效地选择合适的连接顺序。</p>
<h3 id="AlphaJoin2-0"><a href="#AlphaJoin2-0" class="headerlink" title="AlphaJoin2.0"></a>AlphaJoin2.0</h3><p>在上图中的图(c)中可以发现，并非所有测试样例AlphaJoin1.0的性能都比PostgreSQL的查询优化器要好，根据统计数据，由PostgreSQL的查询优化器在其各自的执行引擎上优化的查询中，仍然有大约48%的性能比AlphaJoin1.0的性能要好，我们认为是UCT算法的不确定性造成的这一结果。</p>
<p>一个有趣的发现是，尽管这两种方法在首选查询的数量上是相似的，但AlphaJoin 1.0与PostgreSQL的join枚举过程相比，实现了极大的性能改进，换句话说AlphaJoin在执行时间更长的查询计划上表现得更好，这是因为这些缓慢的查询有更多的空间进行优化。相比之下，对于快速查询的优化，AlphaJoin1.0并不适用。为了进一步提升AlphaJoin1.0的性能，我们提出了AlphaJoin2.0。</p>
<p>为了解决上述问题，我们训练另一种称为自适应决策网络( Adaptive Decision Network, ADN)在AlphaJoin 1.0和PostgreSQL优化器之间进行选择，如图2所示。ADN是从PostgreSQL优化器和AlphaJoin 1.0的历史执行时间的标记数据集学习的。ADN与OVN唯一的区别是输入和输出，输入是SQL-encoding，输出是一个二分类结果，它指示应该选择执行哪个优化器。这个架构我们称之为AlphaJoin2.0。</p>
<h3 id="初步结果"><a href="#初步结果" class="headerlink" title="初步结果"></a>初步结果</h3><p>AlphaJoin2.0与PostgreSQL、NEO、AlphaJoin1.0的性能对比如图3中的图(d)所示，可以看到AlphaJoin2.0拥有更好的性能，PostgreSQL表现更好的测试用例比例从48%降低到了9%。</p>
<h2 id="结论与接下来的工作"><a href="#结论与接下来的工作" class="headerlink" title="结论与接下来的工作"></a>结论与接下来的工作</h2><p>在本文中，我们介绍了AlphaJoin，这是第一个将MCTS用于查询优化来产生高效的表连接顺序的数据库优化器。AlphaJoin包含两个神经网络和MCTS，初步的结果表明它的性能由于NEO（目前最先进的方法）和PostgreSQL的优化器。</p>
<p>接下来，我们计划研究提高OVN和ADN两种网络预测精度以及进一步优化MCTS的搜索效率。此外，我们的方法在一组快速查询上的性能仍然不如PostgreSQL中的优化器，我们需要进一步研究，以进一步提高整体性能，并提出AlphaJoin 3.0</p>
<h2 id="REFERENCES"><a href="#REFERENCES" class="headerlink" title="REFERENCES"></a>REFERENCES</h2><p>[1] Benoit Dageville et al. Automatic sql tuning in oracle 10g. In <em>VLDB</em>, pages 1098–1109, 2004.</p>
<p>[2] Surajit Chaudhuri et al. Self-tuning database systems: A decade of progress. In <em>VLDB</em>, pages 3–14, 2007.</p>
<p>[3] Dana Van Aken et al. Automatic dbms tuning through large-scale machine learning. In <em>SIGMOD</em>, 2017.</p>
<p>[4] Ji Zhang et al. An end-to-end automatic cloud database tuning system using deep reinforcement learning. In <em>SIGMOD ’19</em>, page 415–432, 2019.</p>
<p>[5] Guoliang Li et al. Qtune: A query-aware database tuning system with deep reinforcement learning. <em>Proc.</em> <em>VLDB Endow.</em>, 12(12):2118–2130, 2019.</p>
<p>[6] Ryan Marcus et al. Deep reinforcement learning for join order enumeration. aiDM’18. ACM, 2018.</p>
<p>[7] Ryan Marcus. Towards a hands-free query optimizer through deep learning. In <em>CIDR</em>, pages 1–8, 2019.</p>
<p>[8] Ryan Marcus et al. Neo: A learned query optimizer. <em>Proc. VLDB Endow.</em>, 12(11):1705–1718, 2019.</p>
<p>[9] R´emi Coulom. Efficient selectivity and backup operators in monte-carlo tree search. In <em>Computers</em> <em>and Games</em>, volume 4630, pages 72–83. Springer, 2006.</p>
<p>[10] David Silver et al. Mastering the game of go with deep neural networks and tree search. <em>Nature</em>, 2016.</p>
<p>[11] Jennifer Ortiz et al. Learning state representations for query optimization with deep reinforcement learning. In <em>DEEM’18</em>. ACM, 2018.</p>
<p>[12] Nitish Srivastava et al. Dropout: A simple way to prevent neural networks from overfifitting. <em>J. Mach.</em> Learn. Res.*, 15(1):1929-1958, January 2014.</p>
<p>[13] Ian Goodfellow et al. <em>Deep Learning</em>. MIT Press, 2016.</p>
<p>[14] Sylvain Gelly et al. Exploration exploitation in go: Uct for monte-carlo go. In <em>NIPS Workshop OTEE</em>, 2006.</p>
<p>[15] Viktor Leis et al. How good are query optimizers, really? <em>Proc. VLDB Endow.</em>, 9(3):204–215, 2015.</p>
<p>[16] Anji Liu. Watch the unobserved: A simple approach to parallelizing monte carlo tree search, 2018.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2021/03/17/AlphaJoin%20%E8%BF%9E%E6%8E%A5%E9%A1%BA%E5%BA%8F%E9%80%89%E6%8B%A9%E5%99%A8/" data-id="ckmcwaa5x0001rksqcep0aod4" data-title="AlphaJoin" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-《警惕思维僵化》" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/03/17/%E3%80%8A%E8%AD%A6%E6%83%95%E6%80%9D%E7%BB%B4%E5%83%B5%E5%8C%96%E3%80%8B/" class="article-date">
  <time class="dt-published" datetime="2021-03-17T03:36:40.007Z" itemprop="datePublished">2021-03-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/03/17/%E3%80%8A%E8%AD%A6%E6%83%95%E6%80%9D%E7%BB%B4%E5%83%B5%E5%8C%96%E3%80%8B/">警惕思维僵化</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="《警惕思维僵化》"><a href="#《警惕思维僵化》" class="headerlink" title="《警惕思维僵化》"></a>《警惕思维僵化》</h1><h2 id="经过"><a href="#经过" class="headerlink" title="经过"></a>经过</h2><p>​        昨天中午实验室的师兄准备去干饭，剩下我一个人在实验室，师兄扔了把前门的钥匙给我，叫我出去的时候把实验室门锁好，我当时正在忙着看文献就随口答应了下来。<br>​        继续摸鱼二十多分钟后，我也准备去干饭了，当我从前门出去的时候我意识到师兄只给了我前门的钥匙，后门怎么锁是个*<em>问题(</em>)**。我一开始以为钥匙是通用的，就锁上前门后跑到后门准备用钥匙锁门，发现钥匙插不进去，于是我愣了一下后，下意识地以为师兄忘记给我后门的钥匙，就把后门带上后干饭去了。</p>
<p>​        今天中午休息吹牛的时候，师兄突然问我昨天中午走的时候怎么没有锁后门，我满脸疑惑地说你只给了我前门的钥匙，锁不了后门**(直到这时我还认为师兄是忘记给我后门的钥匙)**。师兄听了我的话后也愣了一下，笑着说你可以先在实验室内把后门反锁了再出去用钥匙锁上前门啊。听了师兄说的话之后，我突然觉得非常有道理，而且应该是很容易想到的一种方式，为什么当时我就没想到呢。</p>
<h2 id="反思"><a href="#反思" class="headerlink" title="反思"></a>反思</h2><p>​        这件事情只是生活中的一个微不足道小事件，但却让我思考了很久。我自认为是一个比较聪明的人，但却没想到锁门的方法。如果你看到**<em><strong>位置时经过思考后也跟我有一样的疑问，那也许你也跟我有一样的问题——</strong>思维僵化</em>*。在长期跟代码打交道的日子里，我逐渐习惯了机器解决问题的死板但又稳定的方式。仔细回想了一下昨天我的心路历程，从得到师兄给的锁门任务，到提取钥匙是锁前门的关键信息，到最后离开实验室执行任务，我似乎没有自己的思考过程(即使只是锁门这样一个很寻常的事情)，遇到不能锁后门的问题时，我得出的结论是师兄忘记给我锁后门的钥匙了，没有去想怎么能同时锁上两个门——这是一个很简单的问题。我的思维似乎就跟计算机程序一样，给定任务和输入，我就循规蹈矩地去执行，遇到问题没有自主的解决方式。</p>
<p>​        。在能轻易获取信息的当今，我经常遇到一些问题就Google搜索，看到别人的回答，就以为这是正确的并深信不疑，少了很多思考和质疑。有可能只是昨天看文献看魔怔了，或是单纯的疲倦了，这件事却在我心里敲响了警钟，警惕思维僵化，保持自己的思考和质疑。</p>
<p>​                                                                                                                                <em>—2020年11月20日</em></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2021/03/17/%E3%80%8A%E8%AD%A6%E6%83%95%E6%80%9D%E7%BB%B4%E5%83%B5%E5%8C%96%E3%80%8B/" data-id="ckmcwaa610002rksq9c0z91z0" data-title="警惕思维僵化" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-Stable-Learned-Bloom-Filters-for-Data-Streams" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/03/17/Stable-Learned-Bloom-Filters-for-Data-Streams/" class="article-date">
  <time class="dt-published" datetime="2021-03-17T03:36:40.007Z" itemprop="datePublished">2021-03-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/03/17/Stable-Learned-Bloom-Filters-for-Data-Streams/">Stable Learned Bloom Filters for Data Streams</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="Stable-Learned-Bloom-Filters-for-Data-Streams"><a href="#Stable-Learned-Bloom-Filters-for-Data-Streams" class="headerlink" title="Stable Learned Bloom Filters for Data Streams"></a>Stable Learned Bloom Filters for Data Streams</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>Bloom过滤器及其变体是用于近似集成员关系查询的数据结构，它具有优雅的、空间利用率高的特点。最近的研究表明，Bloom过滤器的空间成本可以通过与预先训练的机器学习模型的结合显著降低，命名为Learned Bloom Filters(LBF)。LBF通过使用分类器承担部分查询，减轻了Bloom过滤器的空间需求。然而，当前的LBF结构通常以静态成员集合为目标，当集合上有成员更新时，它们的性能将不可避免地下降，而这种更新需求在现实世界的数据流应用程序(如重复项检测、恶意URL检查和web缓存)中并不少见。为了使LBF适应数据流，我们提出了Stable Learned Bloom Filters (SLBF)，通过将分类器与可更新的备份过滤器相结合的方式来解决密集插入工作负载上的性能衰减问题。具体地说，我们提出了两种SLBF结构，单SLBF (s-SLBF)和分组SLBF (g-SLBF)。对这两种结构的理论分析表明，SLBF的期望假正例率(FPR)随着新构件的加入渐近地成为一个常数。在真实数据集上的大量实验表明，SLBF引入了类似级别的假反例率(FNR)，但与针对数据流优化的最先进的(非学习的)Bloom过滤器相比，它有更好的FPR/存储权衡。</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>Bloom过滤器是一个简单、空间利用率高的概率数据结构，用于解决成员查询问题，也就是说查询一个元素$x$是否在集合中<em>S</em>中。由于它的重要性，关于Bloom过滤器及其应用的优化和变体，特别是在数据库和网络领域，在过去几十年里已经进行了大量的研究。尽管Bloom过滤器在学术界和工业界都得到了很好的拓展和评价，但最近的一项提议”The Case for Learned Index structure”中提出，神经网络等机器学习模型可以与B-tree、hash table、Bloom filters等传统索引结构相结合，进一步提高空间利用率和查询效率。</p>
<p>Kraska 等人在他们的开创性工作中提出，成员关系查询可以看作是集合${(x_i,y_i=1)|x_i∈S} ∪{(x_i,y_i=0)|x_i∈N}$上分类问题的一个实例，其中<em>S</em>和<em>N</em>是成员和非成员的集合。它们首先使用一个分类器将元素分类为成员/非成员，然后，为了消除FNR，在集合$S_N = {x|x∈S,x\ is\ predicted\ as\ non-member}$上构建一个小的备用Bloom过滤器，以区分真成员和预测的非成员，这种数据结构即是LBF，如下图所示</p>
<p><img src="/images/Stable-Learned-Bloom-Filters-for-Data-Streams/image-20201222165502920.png" alt="图1"></p>
<p>对于查询处理，如果分类器和备用过滤器都将查询的元素确定为非成员，认定该元素为非成员。例如图中展示的这个例子，集合$S={x,y,z}$，<em>x</em>被分类器正确地预测为成员，因此不需要进一步处理。对于被错误分类的<em>y</em>和<em>z</em>，我们将它们插入标准的Bloom过滤器中，该过滤器包含1个16位的数组和3个哈希函数。当用元素<em>w</em>查询构造的LBF时，假设分类器判定<em>w</em>为非成员，<em>w</em>将在备份过滤器上进一步测试，最终得到非成员决策，即<em>w</em> $\notin$<em>S</em>。</p>
<p>类似于标准BF, LBF也有片面的错误即FPR，与非学习过滤器相比，LBF的优点是：在一个静态元素集合上，它占用更小的存储空间，同时也能具有较高的查询效率和较低的错误率，原因是LBF的空间耗费来自于存储分类器和备份过滤器，但是$S_N$(用于构建备份过滤器的集合)相对较小，存储一个分类器通常只需要较小的空间。</p>
<p>然而，与标准BF类似，LBF是为静态元素集<em>S</em>设计的，其总基数是预先知道的，因此，当有新的元素时由于备份过滤器的结构决定了备份过滤器的空间有限，插入过滤器后，FPR必然增大。与标准BF相比，LBF因插入元素而造成的性能衰减效应更为严重，因为备份过滤器通常较小。</p>
<p>下图说明了这个问题</p>
<p><img src="/images/Stable-Learned-Bloom-Filters-for-Data-Streams/image-20201222201107304.png" alt="图2"></p>
<p>在假设新插入的元素来自与<em>S</em>相同的分布的情况下，通过统计FPR与插入数量的关系发现，在所有四种设置下，新插入80K后，FPR趋于100%。此外，虽然较小的$F_n$可以减缓FPR的增长，但它引入一个更高的初始FPR。直观上，这是因为$F_p$和$F_n$通常相互矛盾，而且Fp是FPR的下界。</p>
<p>虽然LBF空间效率高，但它只适用于预定义的元素集合和仅查询的工作负载，这限制了它在现实世界中的应用，并促使我们设计新的可用于插入密集型工作负载的学习过滤器。但是，构建这样一个支持插入的LBF并不简单，现有的解决动态插入问题的工作，所有目标标准BF，由于存在额外的分类器，它们不能直接应用于LBF上下文。主要的挑战在于以下4个方面：</p>
<ol>
<li>ML模型通常是不确定的，因此，我们需要设计一个新的数学模型来分析学习过滤器在数据流上的性能。</li>
<li>不同于标准BF，我们希望控制FPR，但当滤波器应用于一个存储空间有限的无界数据流时，不可避免地会引入FPR。因此，我们需要仔细量化FPR，并在FPR、FNR和存储之间实现适当的权衡。</li>
<li>当处理静态元素集时，分类器的过拟合通常是好的，因为它改善了$F_p$和$F_n$，但这与流数据的情况相反，因为我们希望分类器能很好地概括将来的元素。</li>
<li>在动态插入上下文中，参数设置变得更加困难。原来在静态元素集合上的LBF，备份过滤器的参数例如哈希函数的数目，是根据元素集的大小、分类器的性能和用户提供的FPR阈值进行优化的，该阈值会随着新元素插入过滤器而改变。</li>
</ol>
<p>为了处理新元素的动态插入，本文设计了一种新的支持插入的LBF结构：Stable LBF(SLBF)。SLBF有以下特性：</p>
<ol>
<li>性能衰减效得到控制，即使大量插入，FPR也有一个非平凡的上界。</li>
<li>在相同的错误率下，总存储花费是有限的，比使用标准过滤器的存储花费更低。</li>
<li>成员查询与标准过滤器一样有效。</li>
</ol>
<p>下图展示了SLBF的特性。</p>
<p><img src="/images/Stable-Learned-Bloom-Filters-for-Data-Streams/image-20201223145156086.png" alt="图3"></p>
<p>左边是在相同存储花费下FPR v.s. #insertions，右边是在相同的FPR期望上界下的性能特征。当应用于动态插入时，SLBF具有较低的存储花费，并能解决性能衰退问题，SLBF可以用于许多真实世界的应用，如重复检测，IP流量监控和搜索引擎优化。据我们所知，这是第一个考虑在动态插入工作负载上优化LBF的工作，我们所作的工作总结如下：</p>
<ol>
<li>介绍了两个新的Bloom过滤器，Simple Stable learned Bloom Filter(s-SLBF)和分Grouping  Stable learned Bloom Filter(g-SLBF)，并实现了上述的三个特性。</li>
<li>我们对我们提出的数据结构在动态插入工作负载下的性能进行了详细的分析，并解释了其参数设置和分类器选择。</li>
<li>我们对真实数据进行了大量的实验研究，结果表明g-SLBF可以有效降低高达97%的存储花费。</li>
</ol>
<h2 id="2-PRELIMINARIES"><a href="#2-PRELIMINARIES" class="headerlink" title="2. PRELIMINARIES"></a>2. <strong>PRELIMINARIES</strong></h2><p>本节首先对标准BF和LBF的结构和分析结果进行概述。然后，我们讨论了无界数据流上BF的稳定性。为了便于快速参考，下图总结了今后使用的所有符号。</p>
<p><img src="/images/Stable-Learned-Bloom-Filters-for-Data-Streams/image-20201223161635853.png" alt="图4"></p>
<h3 id="2-1-Standard-Bloom-Filter"><a href="#2-1-Standard-Bloom-Filter" class="headerlink" title="2.1 Standard Bloom Filter"></a>2.1 Standard Bloom Filter</h3><p>给定一个包含n个元素的集合<em>S</em>，标准BF使用大小为$m$位数组$B$，独立的哈希函数$h_1，···，h_k$将元素映射到<em>1 ~ m</em>。数组<em>B</em>初始化为0，然后，对于每一个$x∈S$，位于$h_0(x)，···，h_k(x)$处的位均设为1。对于元素y的隶属性检验，如果$h_0(y)，····，h_k(y)$所指向的k位均为1，则为正样本，否则为负样本。这样的构造和查询机制确保不存在假负例，但可能存在假正例。在把<em>S</em>的所有n个元素插入位数组<em>B</em>后，<em>B</em>的任意位仍然为0的概率可根据以下公式计算：<br>$$<br>Pr(B[i]=0)=(1-\frac{1}{m})^{kn}\approx e^{-kn}/{m}\qquad\qquad(1)<br>$$<br>用$p0=Pr(B[i]=0)$表示，对于任意非成员元素$y \notin S$，则FPR为：<br>$$<br>FPR_{BF}=Pr(B[h_1(x)]=1\cap\cdots B[h_k(x)]=1)<br>=(1-p0)^k\approx(1-e^{-kn/m})^k\qquad\qquad(2)<br>$$<br>给定n和m，对上式求导，使其导数为0，得到哈希函数的最佳数目是$k^{opt}=\frac{m}{n}ln2$，对应的FPR的最小值是$0.5^{\frac{m}{n}ln2}\approx 0.6185\frac{m}{n}$。更一般地，标准BF的FPR可以建模为$\alpha^t$，其中$\alpha \in (0,1)$，$t=\frac{m}{n}$。</p>
<h3 id="2-2-Learned-Bloom-Filter"><a href="#2-2-Learned-Bloom-Filter" class="headerlink" title="2.2 Learned Bloom Filter"></a>2.2 Learned Bloom Filter</h3><p>LBF的正式定义由Kraska等人提出，Mitzenmacher进一步完善，定义如下：</p>
<p><em><strong>定义1：</strong></em>给定一个包含n个元素的集合$S$，LBF可以用一个三元组$(f,\tau,BF)$表示，其中$f:x \in S \rightarrow [0,1]$是一个预先训练好的分类器，$\tau$是决策阈值，当$f(x) \geq \tau$时，则决策$x \in S$，$BF$是建立在$S$中被错误预测为非成员的所有元素集合上的标准备份BF，即：${x|f(x)&lt;\tau,x\in S}$。</p>
<p>如图1所示，当处理元素y的成员关系查询时，我们相信正样本的预测结果，但质疑来自分类器的负样样本输出，当且仅当分类器和备份过滤器都确定y是非成员时，才认定是负样本。这样的LBF结构保证了单点误差(即没有FN，只有FP)，对于任意非成员元$y \notin S$, FPR为：<br>$$<br>FPR_{LBF} = Pr(f(y)\geq\tau) + Pr(f(y) &lt; \tau)\cdot\alpha^{m/|S_N|}\qquad\qquad(3)<br>$$<br>其中，$m$是备份过滤器的数组大小，$S_n={x|f(x)&lt;\tau,x\in S}$，$\alpha$是一个常数，取决于备份过滤器的实现。对上面这个公式，$Pr(f(y)\geq \tau)$可以解释成分类器的FP率，它本质上是一个随机变量，取决于y是怎么取的，即查询分布。在统计文献中，$Pr(f(y)\geq\tau)$可以通过使用探测数据集来估计，该探测数据集假设是从用于训练分类器$f$的数据集的相同分布中采样的。给定一个LBF ($f,\tau,BF$)，假设分类器$f$和备份过滤器BF分别使用$\zeta$位和$m$位，结合公式(2)和公式(3)，如果下列不等式成立，LBF比使用相同空间(即$\zeta+m$位)的标准BF好：<br>$$<br>F_p+(1-F_p)\cdot\alpha^{b/F_n}&lt;\alpha^{\zeta/n+b}\qquad\qquad(4)<br>$$<br>其中，$F_p$是分类器的FP率，$F_n=|S_N|/n,b=m/n,\alpha$均为常数。该不等式的左右两边分别表示LBF和BF的FPR，两者具有相同的空间和最优的哈希函数个数。</p>
<h3 id="2-3-BF-Stability-on-Data-Streams"><a href="#2-3-BF-Stability-on-Data-Streams" class="headerlink" title="2.3 BF Stability on Data Streams"></a>2.3 BF Stability on Data Streams</h3><p>无论构造标准BF还是LBF，都依赖于对元素集合$S$的全体的了解。为了开始我们关于动态增长元素集上BF稳定性的讨论，我们首先定义数据流上的隶属性测试。</p>
<p><em><strong>定义2：</strong></em>(数据流上的成员查询)考虑一个无约束流的元素$x_1,\cdots,x_n$,n可以无限，查询元素y， y的查询返回true当且仅当 $y\in {x_1,\cdots,x_n}$，即y在时间戳n之前被查询到。</p>
<p>如前所述，任何使用有限空间的BF都不能在无界数据流上实现有界单边错误。直观上，这可以用$FPR=\alpha^{m/n}$(在2.1节中的标准BF中定义)这个模型来解释，由模型可知：$lim_{n\rightarrow \infty}FPR=1$。为了使用有限的存储空间实现非平凡的FPR上界，Deng和Rafiei首先引入了SBF的概念，该概念提出了在插入元素时清除随机比特位的想法，以便为未来的元素腾出空间。</p>
<p>SBF表示动态增长的集合，使用一个大小为$m$的计数器数组$SBF[1,\cdots,m]$，而不是像标准BF那样使用bit数组，每个计数器分配$d$位bits，即$SBF[i]$的取值在$0$和$Max=2^d-1$之间。为了插入元素$x$，首先随机选择$P$个计数器，如果它们不为零，则减1。然后，与标准BF类似，得到$K$个独立的哈希函数值$h_1(x),\cdots,h_K(x)$，计数器$SBF[h_1(x)],\cdots,SBF[h_K(x)]$被设定为$Max$。对一个成员查询元素y，SBF返回true当$SBF[h_1(y)],\cdots,SBF[h_K(y)]$均不为0，否则返回false。使用SBF的插入算法和隶属性查询处理如图4a和图4b所示。</p>
<p><img src="/images/Stable-Learned-Bloom-Filters-for-Data-Streams/image-20201224103004372.png" alt="图4"></p>
<p>当将SBF应用于数据流$x_1,\cdots,x_n$时，一个关键的观察结果是，当插入数$n\rightarrow\infty$时，数组中计数器为0的数量趋于一个常数。给定一个有$m$个计数器的SBF，记为$p_0^{(n)}$为插入$n$个元素后值为0的计数器的比例，则$p_0^{(n)}$的极限为：<br>$$<br>\lim_{n \to \infty}(\frac{1}{1+\frac{1}{P(1/K-1/m)}})^{Max}\qquad\qquad (5)<br>$$<br>此外， $p_0^{(n)}-p_n^{(n-1)}\approx\frac Km(1-\frac Km)^n$，这说明$P_0^{(n)}$是指数收敛的。</p>
<p>$p_0^{(n)}$可以解释为在数据流中插入$n$个元素后任意计数器$SBF[i]$为0的概率。因此，对于不在数据流中的任意查询元素y，SBF的FPR为：<br>$$<br>\lim_{n\to\infty}FPR_{SBF}=\lim_{n\to\infty}(1-p_0^{(n)})^K\approx_{m\gg K}(1-(\frac1{1+K/P})^{Max})^K\qquad\qquad(6)<br>$$</p>
<p>对于数据流上的BF来说，这种在大量插入之后到达一个非平凡FPR而不是衰减到1的特性，被称为“稳定的”。</p>
<p>然而，每次随机选择计数器减1操作，导致SBF实现稳定的代价是FN非零，这意味着已经插入的元素$x_i$可能会被SBF错误地判定为非成员。研究表明SBF的FNR不仅与过滤器的参数有关，还与查询分布有关。关于数据流上FNR的细节讨论将在3.3节中介绍。</p>
<h3 id="2-4-Problem-Statement"><a href="#2-4-Problem-Statement" class="headerlink" title="2.4 Problem Statement"></a>2.4 Problem Statement</h3><p>我们已经概述了标准BF、LBF和SBF的结构和分析结果。在静态元素集合上，与已经使用了几十年的标准BF相比，LBF算法在降低内存开销方面具有优势。这启发我们设计一种新的LBF结果，用于动态元素集合上的近似成员查询(如定义2所示)。</p>
<p>具体地说，我们考虑这种数据结构的两种操作：</p>
<ol>
<li>将元素$x$插入到过滤器中。</li>
<li>使用过滤器返回元素y的成员关系测试结果的查询。</li>
</ol>
<p>当应用于数据流时，期望该LBF能达到稳定特性(即$n\rightarrow\infty$时FPR达到非平凡值)，以及与在数据流上优化的非学习过滤器(如SBF)相比在相同FPR/FNR级别上消耗更少的存储空间。</p>
<h2 id="STABLE-LEARNED-BLOOM-FILTER"><a href="#STABLE-LEARNED-BLOOM-FILTER" class="headerlink" title="STABLE LEARNED BLOOM FILTER"></a>STABLE LEARNED BLOOM FILTER</h2><p>在本节中，我们将介绍两种数据结构(第3.1节和第3.2节)以及理论分析(第3.3节)，以解决在学习索引背景下的流数据的近似隶属性测试问题。</p>
<h3 id="3-1-Single-SLBF"><a href="#3-1-Single-SLBF" class="headerlink" title="3.1 Single SLBF"></a>3.1 Single SLBF</h3><p>为了使原始的LBF框架在无限次插入后任然保持稳定，一个直观的想法是用一个稳定的BF(2.3节中介绍)取代LBF中的标准备份过滤器。如图5a所示，这种结构称为single stable Learned Bloom filter(s-SLBF)，其中single表示在该框架中有一个备份过滤器。</p>
<p><em><strong>定义3：</strong></em>一个s-SLBF可以表示成一个三元组$(f,\tau,SBF)$，其中$f$是一个预先训练好的分类器，$\tau$是相应的决策阈值，$SBF$是稳定备份过滤器。</p>
<p><img src="/images/Stable-Learned-Bloom-Filters-for-Data-Streams/image-20201224132615062.png" alt="图5"></p>
<p>插入一个新的元素$x$时(如图4c)，首先计算$f(x)$，并与阈值$\tau$比较，若$f(x)&gt;\tau$，则说明模型判断$x$已经预测为集合成员，插入过程立即终止；否则，$x$被插入$SBF$。查询元素y(如图4d)时，若$f(y)\geq \tau$或者$f(y)&lt;\tau$但$SBF$判断$y$是正样本，则返回正样本结果。</p>
<p>虽然与原始LBF非常相似，但s-SLBF中的分类器(即$f$和$\tau$)获得的方式本质上是不同的。回顾原始LBF分类器$f$的构造，是通过一个二分类数据集${x_i,y_i=1|x_y\in S}\cup{x_i,y_i=0|x_i\in N}$，其中$S$是构造过滤器的集合，它是静态的，$N$是由负样本组成的集合。相比之下，对于s-SLBF，由于它是在数据流进入之前建立的，而不是精确的元素集合(即$S$)，因此可以利用先验知识来训练分类器。这就产生了LBF和SLBF之间适用场景的根本区别(详细讨论在附录A)。</p>
<p>显然，如果我们假设数据流遵循训练分类器时使用的分布，s-SLBF在大量插入后是稳定的，根据公式(3)和公式(6)，设$SBF$的参数$m,K,P,Max$，则s-SLBF的FPR期望如下：<br>$$<br>E[FPR]=F_p+(1-F_p)\cdot(1-(\frac1{1+K/P})^{Max})^K\qquad\qquad(7)<br>$$<br>其中，$F_p=Pr_{y\sim D_N}(f(y)\geq\tau)$，$D_N$是非成员的分布。</p>
<p>如果分类器表现良好，在相同的预期FPR水平下，s-SLBF应该比原始SBF节省更多空间，我们使用下面的例子来解释这个优点。</p>
<p>根据公式(3)，对SBF，因为$m\gg K$，FPR的稳定对$m$不敏感。不失一般性，我们假定s-SLBF分类器的$F_p=0.01,F_n=0.5$，进一步选择SBF的参数$P,K<del>and</del>Max$满足$(1-(1/(1+K/P))^{Max})^K\approx0.1$。在这样的设定下，通过公式(7)，s-SLBF稳定时的FPR上界为$0.01 + 0.99∗0.1\approx0.1$，与使用相同参数的SBF相比处于同一水平。由于SBF的总存储成本是$m\cdot \lfloor log_2(Max) + 1\rfloor$ ，唯一影响总存储开销的因素是计数器的数量$m$($Max$是固定值)。由公式(5)和公式(6)，对SBF，$m$的增加或减少不影响稳定时的FPR，但会影响FPR收敛到稳定点的速度。因此，为了公平起见，我们比较了s-SLBF和SBF在相似稳定FPR和收敛速度下的存储开销差异。假设SBF和s-SLBF使用的计数器数分别为$m$和$m’$，让两个过滤器有相同的收敛速率，我们得到下面的方程：<br>$$<br>\frac Km(1-\frac Km)^N=\frac {K}{m’}(1-\frac{K}{m’})^{N\cdot F_n}\qquad\qquad(8)<br>$$<br>通过设定参数$m=10^6,K=6,N=10^8,F_n=0.5$，我们可以解得$m’\approx4.7\times10^4$，这表明存储开销降低了约53%。注意，我们忽略了s-SLBF中分类器造成的空间开销，因为它通常比计数器数组小得多。</p>
<p>为了进一步理解公式(8)中$m’$和$F_n$之间的关系，我们将$F_n$取值$0.3$到$0.5$,$N$取值为$5\times10^4$到$10^7$，结果如图6。随着$N$的增加相邻线之间的间隙减小，这表明当插入的数量$N$大大增加时，过滤器接近稳定点。我们可以观察到$m’$和$F_n$之间的近似线性关系，这是合理的，因为$F_n$决定有多少插入流中的元素从分类器“转义”并添加到备份SBF。分类器的$F_n$越高，意味着需要向备份SBF插入更多的元素，因此需要更多的计数器来保持相似的收敛速度。注意，上面的模拟修正了$F_p$等其他参数以简化分析，并提供对LBF优势的总体了解。</p>
<p><img src="/images/Stable-Learned-Bloom-Filters-for-Data-Streams/image-20201224165522871.png" alt="图6"></p>
<h3 id="3-2-Grouping-SLBF"><a href="#3-2-Grouping-SLBF" class="headerlink" title="3.2 Grouping SLBF"></a>3.2 Grouping SLBF</h3><p>作为一个直接扩展，s-SLBF已经被证明是稳定的，使用的存储空间可能比我们预期的SBF更低。然而，它也继承了原始LBF框架的一个主要缺点，即相信分类器做出的所有正样本预测，种过度依赖使得s-SLBF和原始LBF在分类器不可靠时变得脆弱。这种现象可以用公式(7)来解释，其中，分类器的$F_p$是整体FPR的下界。</p>
<p>除了对分类器的过度依赖问题，单个备份过滤器的设计方式也遗漏了分类器提供的有用信息。图5a说明了s-SLBF中分类器的工作方式，从中我们可以发现分类器的$F_p$和$F_n$都来自于决策阈值的设置，也就是说，位于左侧的负(正)样本都被归类为正(负)样本。这个生硬的决策规则不区分落在同一侧的元素的置信度，下面这个例子说明了这个问题：</p>
<p>假设决策阈值设为$\tau=0.7$，预测得分分别为0.69和0.01的元素没有差异，两者的处理方法相同，即反馈给同一个备份SBF。类似地，对于预测得分为0.71到0.99的元素，直接做出成员决策而没有反馈给备份SBF。</p>
<p>通过以上分析，我们认识到s-SLBF和原来的LBF的主要缺陷是单一备份过滤器的性质。为了进一步改进s-SLBF，我们引入了第二种数据结构：grouping stable Learned Bloom filter(g-SLBF)，将得分(范围[0,1])分类成几个区间，并为每个区间分配独立的子过滤器。</p>
<p><em><strong>定义4：</strong></em> g-SLBF由一个分类器$f$和$g$个不同的$SBF$(也称为子过滤器)$SBF_1,\cdots,SBF_g$构成，其中第$j$个$SBF$ $SBF_j$由参数$(m_j,K_j,P_j,Max_j)$描述。将区间$[0,1]$划分为$g$个子区间，即$[\tau_0 = 0,\tau_1],[\tau_1,\tau_2 ],\cdots,[\tau_{g-1},\tau_g=1]$，用于将元素$x$映射到$SBF$关于它们的预测值$f(x)$。更具体地说，如果$f(x)\in(\tau_{j-1}，\tau_j]$(如图4e所示)，则向$SBF_j$插入一个新元素$x$。为了测试元素y的成员隶属性，我们直接查询$f(y)$映射到的子过滤器$SBF_j$(如图4f所示)。</p>
<p>如图5b所示，g-SLBF的基本思想是根据分类器给出的隶属性置信度将插入流中的元素划分为几个组。直观地说，对于那些以低隶属性置信度插入的元素(即位于置信度分布的左侧，如图5b所示)，由于分类器在这个范围内$F_p$比较高，我们可以通过适当设置$K, P,Max$，调整相应子过滤器$SBF_j$来补偿FPR的损失。此外,因为$F_n$在这个范围内较低，这意味着不会有太多的元素要插入到$SBF_j$，需要分配的计数器较少，以在令人满意的收敛速度下稳定达到预期的FPR。另一方面，对于高置信度的元素(即位于置信度分布的右侧，如图5b所示)，SBF的FPR要求可以放宽，因为这个范围内的元素已经有很高的可能性是一个成员，同样的，因为分类器可能错误地确定许多成员元素分类为非成员(即高$F_n$)，因此需要更多的计数器让$SBF_j$收敛到稳定点。</p>
<p>值得注意的是，s-SLBF可以看作是g- SLBF的一种特殊情况，通过设置$g=2$，即只有一个决策阈值$\tau$，并且让子过滤器在范围$[\tau,1]$内做出正样本预测。与s-SLBF相比，g-SLBF的正预测是完全可信的，g-SLBF(g &gt; 2)对分类器输出更保守，因为所有的隶属性决策都是由分类器和备份过滤器共同做出的。这样的特性使得g-SLBF对分类器的质量更具有鲁棒性(例如，传入的元素流并不像训练数据那样严格遵循分布)。g-SLBF在鲁棒性方面的优势将在接下来的章节中通过分析和实验来展示。</p>
<h3 id="3-3-Analytical-Results"><a href="#3-3-Analytical-Results" class="headerlink" title="3.3 Analytical Results"></a>3.3 Analytical Results</h3><p>在本节中，我们分析了两种SLBF结构的FPR、FNR和收敛性。注意，我们之所以关注g-SLBF，是因为s-SLBF是g-SLBF的一种特殊情况，其理论结果自然适用于s-SLBF。我们首先在下面给出一些初步的记号。</p>
<p>对于g-SLBF中的第$j$个分类分数区间$[\tau_{j-1},\tau_j]$，我们定义两种概率$p_j$和$q_j$:<br>$$<br>p_j=Pr_{x\in D_N}(f(x)\in[\tau_{j-1},\tau_j]),\qquad\qquad \<br>q_j=Pr_{x\in D_P}(f(x)\in[\tau_{j-1},\tau_j])\qquad\qquad(9)<br>$$<br>其中，$D_N$和$D_P$是非成员和成员的分布。$(p_j,q_j)$描述了分类器在$[\tau_{j-1},\tau_j]$范围内的假阳性和假阴性行为。需要注意的是，在$D_N$和$D_P$未知的情况下，一般很难知道$p_j$和$q_j$的确切值。然而，如参照参数设置(第4节所示)，我们可以使用测试数据集来估计$p_j$和$q_j$。</p>
<p>在接下来的分析中，我们对分类器和数据采用了以下两个假设，这两个假设不难理解，并且已经被现有的LBF工作所采用。</p>
<p>**假设1.**过滤器的成员和非成员分别遵循分布$D_P$和$D_N$。因此，一个SLBF的FPR是在非成员分布$D_N$上的预期假阴性率(即在$D_N$上预测错误的预期概率)。</p>
<p><strong>假设2</strong>.对$j=1,\cdots,g$，有$p_1\ge p_2\ge \cdots \ge p_g$和$q_1\le q_2 \cdots \le q_g$。</p>
<h4 id="3-3-1-False-Positive-Rate-and-Stability"><a href="#3-3-1-False-Positive-Rate-and-Stability" class="headerlink" title="3.3.1 False Positive Rate and Stability"></a><em>3.3.1 False Positive Rate and Stability</em></h4><p>假设一个由n个元素$x_1,\cdots ,x_n$组成的序列被插入到g-SLBF中，其中$x_i\sim D_P$，对于来自非成员分布$D_N$中的元素的新查询，g-SLBF(稳定时)的期望FPR为：<br>$$<br>E[FPR]=\sum^{g}_{j=1}p_j\cdot(1-(\frac{1}{1+K_j/P_j})^{Max_j})^{K_j}\qquad\qquad(10)<br>$$<br>其中，$(1-(\frac{1}{1+K_j/P_j})^{Max_j})^{K_j}$可以表示为$\alpha_j$，假设有满足$\alpha_1\le \alpha_2  \le \cdots \le \alpha_g$的$g$个SBF。然后考虑<strong>假设2</strong>中描述的间隔，<strong>引理1</strong>描述了如何分配SBF的这些间隔来最小化$E[FPR]$，这也验证了我们在3.1节中的讨论。</p>
<p>**引理1.**分配过滤器中$\alpha_j$的间隔$[\tau_{j-1},\tau_j],j=1,\cdots,g$可以最小化$E[FPR]$。</p>
<p><em>证明：</em>根据假设有$p_1\ge p_2\ge \cdots \ge p_g$和$\alpha_1\le \alpha_2  \le \cdots \le \alpha_g$，根据重排不等式，对于任何其他排列$\alpha_{\sigma(1)},\alpha_{\sigma(2)}\cdots\alpha_{\sigma(g)}$，<br>$$<br>\sum^{g}<em>{j=1}p_j\cdot\alpha</em>{\sigma(j)}\ge\sum^{g}_{j=1}p_j\cdot\alpha_j=E[FPR]\qquad\qquad(11)<br>$$<br>在<strong>引理1</strong>的基础上，证明了g-SLBF(稳定)的期望FPR的一个上界，该上界不受$p_j$的约束。</p>
<p>**定理1(FPR上界定理).**g-SLBF在稳定状态下的期望FPR的上界是$g$个子过滤器$SBF_1,\cdots，SBF_g$的FPR的算术平均值，即$E[FPR] \le \frac {1}{g} \sum^{g}_{j=1} \alpha_j$。</p>
<p><em>证明：</em>因为$p_1\ge p_2\ge \cdots \ge p_g$和$\alpha_1\le \alpha_2  \le \cdots \le \alpha_g$，根据Chebyshev不等式和，下列式子总是成立的：<br>$$<br>E[FPR]=\sum^{g}<em>{j=1}p_j\cdot \alpha_j \le g \cdot (\frac{1}{g} \sum^{g}</em>{j=1}p_j)\cdot(\frac{1}{g} \sum^{g}<em>{j=1}\alpha_j)=1 \cdot \frac{1}{g} \sum^{g}</em>{j=1} \alpha_j=\frac{1}{g} \sum^{g}_{j=1} \alpha_j\qquad\qquad(12)<br>$$<br>回顾3.2节，我们认为g-SLBF中分类器质量的鲁棒性比s-SLBF强，正如我们之前讨论的， s-SLBF(以及原来的LBF)采用单一备份过滤器结构，从而使分类器的FPR直接上界于整体FPR。然而，通过<strong>定理1</strong>，g-SLBF的FPR以子过滤器的FPR的算术平均值的为界限，它独立于分类器的质量和特定的分布假设。注意，上述不等式的成立条件是假设$p_1\ge p_2\ge \cdots \ge p_g$，如果数据集是”可学习的”，这通常是成立的(参见我们在5.3节对这个假设的验证)。我们还进行了实验研究，通过增加元素流分布的畸变来验证鲁棒性，结果表明g-SLBF的FPR恶化速度远慢于s-SLBF，更多细节见附录E。</p>
<h4 id="3-3-2-Convergence-Rate"><a href="#3-3-2-Convergence-Rate" class="headerlink" title="3.3.2 Convergence Rate"></a><em>3.3.2 Convergence Rate</em></h4><p>下面的定理描述了g-SLBF的收敛速度，即过滤器接近稳定FPR的速度有多快。</p>
<p>**定理2(G-SLBF收敛定理).**g-SLBF以$O(exp(-C\cdot n))$的速率收敛于其稳定点，如公式(10)所示，其中$n$是总的插入数量，$C=min_j\frac{q_jm_j}{K_j},j=1,\cdots,g$。</p>
<p><em>证明：</em>如2.3节所介绍的，子过滤器$SBF_j$的收敛速度为：<br>$$<br>\frac{K_j}{m_j}(1-\frac{K_j}{m_j})^{q_j\cdot n}=\frac{K_j}{m_j}(1-\frac{K_j}{m_j})^{\frac{K_j}{m_j} \cdot \frac{m_jq_jn}{K_j}}\approx O(exp(-\frac{q_j}{k_j}n))\qquad\qquad(13)<br>$$<br>显然，当且仅当$SBF_1,\cdots,SBF_g$都是稳定的，g-SLBF才能达到稳定点，因此，总体收敛速度为最慢子过滤器的收敛速度，即$O(exp(-n\cdot min_j \frac{q_jm_j}{K_j}))$。</p>
<h4 id="3-3-3-False-Negative-Rate"><a href="#3-3-3-False-Negative-Rate" class="headerlink" title="3.3.3 False Negative Rate"></a><em>3.3.3 False Negative Rate</em></h4><p>当对成员的查询给出否定答案时，就会出现假阴性，即一个此前已经插入的元素被判断为非成员。于SBF类似，我们的g-SLBF允许大量的假阴性，以在无限制数据流上用有限的存储来实现有界的FPR(稳定)。为了量化假阴性对我们的数据结构的影响，我们首先回顾构成我们g-SLBF子的子过滤器SBF的FNR。</p>
<p>与仅由过滤器参数决定的FPR不同，SBF的FNR还依赖于输入数据流和查询工作量的特征。给定一个数据流中的元素$x_i$，设$\delta_i$为元素$x_i$最近一次查询和插入之间的次数，称为$x_i$的间隙。对于插入的元素$x_i$，元素$x_i$的假阴性概率为：<br>$$<br>Pr(FN_i)=1-\prod^K_{j=1}(1-Pr(SBF[h_j(x_i)]=0|\delta_i))\qquad\qquad(14)<br>$$<br>其中，$Pr(SBF[h_j(x_i)]=0|\delta_i)$表示计数器$SBF[h_j(x_i)]$在$\delta_i$次插入后变为0的概率。注意，如果$\delta_i&lt;Max$，那么$Pr(SBF[h_j(x_i)]=0|\delta_i)$总是0，因为计数器不可能减到0(回顾图4a中描述的插入SBF算法)。</p>
<p>对我们的g-SLBF，它采用相互独立的SBF序列做为子过滤器，假设在第$j$个子过滤器$SBF_j$中插入一个元素$x_i$，根据公式(14)，其假阴性概率为：<br>$$<br>Pr(FN_i|x_i\in SBF_j)=1-(1-p_N(\delta_i,k_{ij}))^{K_j}\qquad\qquad(15)<br>$$<br>其中，$p_N(\cdot)$表示$K_j$个过滤器($x_i$所映射)其中一个在$\delta_i$次插入操作已经减到0的概率($\delta_i是x_i的间隙$)。$p_N(\cdot)$是$\delta_i$和$k_{ij}$的函数，计算计数器被设置为$Max_j$的概率是多少。注意，$k_{ij}$是一个随机变量，它取决于数据流中每个元素的出现频率，也就是说，每个$x_i$的$p_N(\cdot)$都是不同的，这使得我们很难精确计算FNR,因为我们对这些插入频率没有先验知识。另一方面，在数据流足够大的情况下($n\to\infty$)，这种频率特征对整体的FNR结果影响不大。因此，在我们的工作中，在不失一般性的情况下，我们假定一个元素在插入数据流中只出现一次，即不存在重复插入，从而得到g-SLBF的期望FNR。</p>
<p>在上述假设下，每个$x_i$对应的$k_{ij}$都是相同的形式，即$k_{ij}=k_j=\frac{1}{nj}(1+\sum^{n_j-1}<em>{l=1}I_l)$，其中$n_j$是已经插入到过滤器$SBF_j$和的元素的数量，$I_l$是一个伯努利分布随机变量，满足$Pr(I_l=1)=K_j/m_j$。因此，g-SLBF的总体期望FNR可以被推导为：<br>$$<br>E[FNR]=\sum^g</em>{j=1}Pr(FN|x\in SBF_j)\cdot Pr(x\in SBF_j)=\sum^g_{j=1}(1-(1-p_N(\tilde\delta,k_j))^{K_j})\cdot q_j\qquad\qquad(16)<br>$$<br>其中，$\tilde\delta$是数据流的平均间隙。$p_N(\cdot)$基于$\tilde\delta$和$k_j$的具体评价请参见附录C。一旦$p_N$和过滤器参数确定，我们就可以利用上式估计FNR。</p>
<p>综上所述，在我们的g-SLBF上，作为在无限次插入后任然具有稳定性的一个副作用，假阴性是不可避免的，这与SBF相似。与FPR不同的是，g-SLBF的FNR的确定依赖于插入元素流和查询元素流的先验知识(计算每个插入元素$x_i$的间隙值$\delta_i$)。为了解决假阴性问题，在接下来的部分中，我们设计了一个参数设置策略，目标是最小化FNR，同时将FPR限定在一个用户给定的阈值内。第5节给出的详细评价结果表明，我们的g-SLBF与SBF相比有相似的FNR，但达到更好的FPR/存储比，即在相同的FNR和FPR水平下，我们所提出的学习过滤器可以节省更多的存储空间。</p>
<h4 id="3-3-4-Time-Complexity"><a href="#3-3-4-Time-Complexity" class="headerlink" title="3.3.4 Time Complexity"></a><em>3.3.4 Time Complexity</em></h4><p>使用g-SLBF进行插入操作和成员查询处理都需要$O(1)$时间。假设模型预测时间为$O(M)$，则插入和查询处理分别是$O(M+max_j(P_j+K_j))$和$O(M+max_j(K_j))$，因为所有的$M,P_j,K_j$都是指定的常量，我们的结论是，使用g-SLBF进行插入和隶属性测试需要常数时间。</p>
<h2 id="4-PARAMETER-SETTING"><a href="#4-PARAMETER-SETTING" class="headerlink" title="4. PARAMETER SETTING"></a>4. PARAMETER SETTING</h2><p>在这一节，我们将讨论如何根据分析的结果来正确设置g-SLBF的参数。同样，在不失一般性的情况下，我们将重点放在g-SLBF上，并且参数设置策略可以自然地扩展到s-SLBF。</p>
<p>之后再翻译，现在看这个意义不大</p>
<h2 id="5-EXPERIMENTAL-STUDY"><a href="#5-EXPERIMENTAL-STUDY" class="headerlink" title="5. EXPERIMENTAL STUDY"></a>5. EXPERIMENTAL STUDY</h2><p>在本节中，我们将报告在真实世界应用程序的数据集上的实现细节和实验结果。所有的实验都是在Intel(R) Core(TM) i7-8550U <a href="mailto:&#x43;&#80;&#x55;&#64;&#x31;&#x2e;&#x39;&#x39;&#x47;&#x48;&#122;">&#x43;&#80;&#x55;&#64;&#x31;&#x2e;&#x39;&#x39;&#x47;&#x48;&#122;</a>，内存16GB的Ubuntu笔记本电脑上进行的，所有的方法都使用C语言实现，使用GCC编译并使用O3参数优化。</p>
<h3 id="5-1-Baselines-and-Implementation-Details"><a href="#5-1-Baselines-and-Implementation-Details" class="headerlink" title="5.1 Baselines and Implementation Details"></a>5.1 Baselines and Implementation Details</h3><p>为了展示我们提供的数据结构的有效性，我们实现并比较了5个过滤器，包括标准BF(BF)，稳定BF(SBF)，传统学习BF(LBF)，简单SLBF(s-SLBF)和分组SLBF(g-SLBF)。</p>
<p><strong>BF和LBF.</strong> BF和LBF是本实验的基线，以展示非稳定过滤器在流数据上的表现。BF的实现遵循最标准的空间优化BF，其中哈希函数的数量总是设置为最优。对LBF，我们使用相对简单的模型(如梯度提升树)来考虑流数据场景中的效率，而不是使用的深度学习模型(细节将在后面讨论)。</p>
<p><strong>SBF,s-SLBF and g-SLBF.</strong> 这三种过滤器在流数据场景下实现了稳定性。SBF的参数和s-SLBF、g-SLBF的参数分别根据参考文献[13]和我们在第4节的讨论设置。</p>
<p><strong>Hash function Implementation.</strong> 所有过滤器都需要计算$K$个哈希值。我们采用xxHash，这是一种极快的高质量非加密的哈希方案。此外，我们不完全计算$K$个独立的哈希值，而是，只计算两个独立的哈希值$h_a,h_b$，并且第$j$个哈希值由$h_a+j*h_b,(j = 1,\cdots,K)$给出。</p>
<p><strong>Classifier Implementation.</strong> 在第一篇学习索引文献[25]中，建议使用深度学习模型，即神经网络模型来构建学习数据结构。具体来说，他们建议使用循环神经网络。尽管深度学习模型在很多任务中都有较好的表现，但是考虑到数据流上的成员查询处理对实施时间的要求，常见的深度学习框架如Tensorflow合pyTorch都过于笨重，难以部署。虽然使用GPU可以缓解推理效率的问题，但在CPU和GPU之间迁移数据可能成为一个新的瓶颈。由于这不是一篇介绍新的机器学习方案的论文，为了追求效率，我们测试并比较了三个轻量级模型：逻辑回归，支持向量机合基于Catboost的梯度提升树(GBT)。我们发现考虑分类器质量，存储开销和推理效率这三个因素GBT分类器表现得足够好，因此我们采用GBT作为LBF和SLBF的分类器。</p>
<h3 id="5-2-Datasets-Parameters-and-Metrics"><a href="#5-2-Datasets-Parameters-and-Metrics" class="headerlink" title="5.2 Datasets,Parameters and Metrics"></a>5.2 Datasets,Parameters and Metrics</h3><p>为了展示我们的SLBF的有效性与效率，我们测试了5种过滤器在三个数据集：Amazon,Attack,Higgs上的工作情况。下面我们简要介绍每个数据集，表2总结了这些数据集的统计信息。</p>
<p><strong>Task 1: Amazon.</strong> 该数据集包含从2010年到2011年收集的亚马逊员工的资源访问记录，在这些记录中，员工在一段时间内被允许或拒绝访问资源。每个记录包含一个唯一的ID和用于构建分类器的10个特征。</p>
<p><strong>Task 2: Attack.</strong> 这是一个网络攻击跟踪数据集。包括报文的源/目的和流量统计，总共提取了23个特征。</p>
<p><strong>Task 3: Higgs.</strong> 这是一个科学数据集，它要求对信号处理过程是否产生希格斯玻色子进行分类。通过粒子检测器得到的运动特征共有28个。通过粒子检测器得到的运动特征共有28个。</p>
<p><img src="/images/Stable-Learned-Bloom-Filters-for-Data-Streams/image-20201229144707596.png" alt="表2"></p>
<p>对每个数据集，采样其中一部分，具体来说是20%，用于估计预训练的分类器以及计算一些参数如$p_j,q_j$(公式(9))，剩下80%的数据用于生成插入和查询流。我们使用梯度提升树模型训练每个任务的分类器，模型信息如表3所示。</p>
<p><img src="/images/Stable-Learned-Bloom-Filters-for-Data-Streams/image-20201229145646750.png" alt="表3"></p>
<p><strong>Insertion Workloads.</strong> 将数据集中所有的正样本作为成员，负样本作为非成员，对一个数据集，相应的插入工作负载是正样本序列。注意，当向非学习的过滤器如BF和SBF插入元素时，我们只将其标识符插入过滤器，元素的特征信息将被丢弃。类似地，对学习过滤器如LBF，s-SLBF和g-SLBF，特征信息将被分类器用于计算隶属性得分，如果有必要，只向过滤器插入标识符。</p>
<p><strong>Query Workloads.</strong> 我们需要查询工作负载来评估所有过滤器的性能。通过第3节的分析，FPR和FNR分别由成员和非成员所测出。此外，FNR还受到插入到过滤器中的成员元素与被查询之间的时间间隔的影响。因此，对每个数据集，给定一个间隙值$\delta$，对于每个插入过滤器的元素$x_i$，我们将在其他元素插入$\delta$次后查询它来测量FNR。插入所有元素之后(来自每个task的正样本集)，我们将使用负样本集查询过滤器来度量FPR。然后将查询工作负载下的经验FNR和FPR计算为$EFNR=\frac { false\ negatives}{positive\ samples}$以及$EFPR=\frac {false\ positives}{negative\ samples}$。注意，在本节报告的结果中，我们设定$\delta$为2,000，这对于实际应用来说是一个合理的值。然而，我们也报告了通过改变$\delta$的结果，结果表明随着$\delta$的增加，对于SBF和我们的SLBF，FNR有明显的增加趋势。这是合理的，因为较高的$\delta$会增加后备过滤器中计数器下降到0的可能性，从而导致假阴性，更多信息参见附录D。</p>
<p><strong>Control variables.</strong> 用3个参数：FPR的上限$\epsilon$，总存储预算$B$，以及g-SLBF的组数$g$来评估过滤器的鲁棒性。表4总结了每个数据集的参数设置，其中下划线值被视为默认值。</p>
<p><img src="/images/Stable-Learned-Bloom-Filters-for-Data-Streams/image-20201229151825465.png" alt="表4"></p>
<h3 id="5-3-Experimental-Results"><a href="#5-3-Experimental-Results" class="headerlink" title="5.3 Experimental Results"></a>5.3 Experimental Results</h3><p><strong>Validation of Assumptions.</strong> 在3.3节分析我们的SLBF的性能时，我们假设一个训练好的分类器满足$p_1\ge p_2\ge \cdots \ge p_g$和$q_1\le q_2 \cdots \le q_g$，为了验证这一假设，我们使用Attack和Higgs的正样本集和负样本集来计算相应的分类得分，并绘制如图10所示的直方图，由此可以验证我们的假设。此外，分数直方图也可以用来指导$g$的设置，因为我们可以一直对区间进行划分，直到这种单调关系不成立为止。</p>
<p><strong>Validation of  Stability.</strong> 为了验证过滤器的稳定性，我们用Amazon数据集测试g-SLBF($g=6,\epsilon=10%,B\ in\ range\ 2^{10}\cdots 2^{18}$)，具体来说，我们在每4000次新插入后使用负样本集来测量经验FPR，并绘制图11中的结果。我们发现EFPR随着新元素的插入增长，最后达到稳定值0.1，此外，$B$越小，过滤器的收敛速度越快。g-SLBF在其他数据集和s-SLBF上的稳定性结果与图11相似。</p>
<p><img src="/images/Stable-Learned-Bloom-Filters-for-Data-Streams/image-20201229170429692.png" alt="图8"></p>
<p><img src="/images/Stable-Learned-Bloom-Filters-for-Data-Streams/image-20201229170443128.png" alt="图9"></p>
<p><img src="/images/Stable-Learned-Bloom-Filters-for-Data-Streams/image-20201229170457285.png" alt="图10"></p>
<p><strong>Overall Comparison.</strong> 本次实验在3个数据集上测试了5种过滤器，通过修改$g$和$\epsilon$的默认值，经验FPR和FNR(使用查询工作负载计算)与比特预算B的比较如图8所示。注意，$B$是指分配给(备份)过滤器，这也是BF和SBF的实际存储成本。对LBF，g-SLBF和s-SLBF，它们的实际存储成本是$B$加上分类器模型的大小。然而，如表3所示，我们可以发现模型大小是恒定的，并且相对较小。更具体地，考虑使BF和LBF产生图8a-8c中的显著FPR的最小$B$，例如，$B\approx2^{20}\ for\ Amazon,B\approx2^{22}\ for\ Attack\ and\ B\approx2^{25}\ for \ Higgs$(，)这意味着当$B$小于这个值时，BF或LBF的FPR都太高且无法忍受)，即使在这种情况下，分类器的成本也只占$B$的0.5% ~ 16%。因此，对于所有的5个过滤器，我们可以简单地把比特预算$B$作为它们的存储成本。</p>
<p>我们首先比较5种过滤器在不同$B$的情况下的FPR，如图8a-8c所示。当B非常小时，BF或LBF的FPR都非常高(接近100%)，但会随着B的增加而下降，这意味着它们需要分配一个相当大的$B$来达到可观的FPR。相比之下，SBF，s-SLBF和g-SLBF的FPR都在5%以内，这是我们学习过滤器的默认设置，即默认FPR的上界。我们可以观察到，在相同存储成本下，我们的g-SLBF的FPR总是优于其他过滤器，相比相同FPR下的存储成本，其优势更加明显。例如，在Attack数据集上，$B=2^{18}$时g-SLBF的FPR和$B = 2^{24}$时SBF的FPR约为0.1%，但是它们的存储成本比为$\frac{2^{18}Bit+173KBits}{2^{24}Bit}\times100%\approx2.6%$，这意味着g-SLBF需要的存储空间仅为SBF的2.6%。对于FNR，如图8-8f所示，随着$B$的增加，SBF，s-SLBF和g-SLBF的表现相似。我们的学习SBF在FNR上的性能似乎没有明显优于SBF。原因是，一个好的分类器可以通过提供一个好的预测分数来降低FPR，但是它对避免错误否定没有太大帮助，因为否定(非成员)决策仅由备份过滤器做出。</p>
<p>此外，我们发现g-SLBF的FPR可以通过分配更多的比特位来不断降低，而s-SLBF则不适用，因为分类器的FPR是s-SLBF整体FPR的下界。这验证了我们的论点，即g-SLBF对分类器输出更保守，因此更稳健。</p>
<p><strong>Varying  $\epsilon$.</strong> 在本实验中，我们研究了FPR上限，参数ε对g-SLBF的影响。我们将g-SLBF的FNR w.r.t. ε绘制在图5-4(a)中，可以清楚地看出ε与FNR呈反比关系，即期望FPR越高，假阴性的数量就越少。这是合理的，因为根据公式(3-6)，较高的ε意味着较低的P和较高的Max，这使得计数器更难减少到0，从而降低FNR。</p>
<p>​    </p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2021/03/17/Stable-Learned-Bloom-Filters-for-Data-Streams/" data-id="ckmcwaa630003rksq9k233wx9" data-title="Stable Learned Bloom Filters for Data Streams" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-hello-world" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/03/17/hello-world/" class="article-date">
  <time class="dt-published" datetime="2021-03-17T03:04:33.264Z" itemprop="datePublished">2021-03-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/03/17/hello-world/">Hello World</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2021/03/17/hello-world/" data-id="ckmcv2zu800007wsq1np94mh5" data-title="Hello World" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/03/">March 2021</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2021/03/17/Neo%E4%B8%80%E4%B8%AA%E5%AD%A6%E4%B9%A0%E5%9E%8B%E6%9F%A5%E8%AF%A2%E4%BC%98%E5%8C%96%E5%99%A8/">(no title)</a>
          </li>
        
          <li>
            <a href="/2021/03/17/PAlab-report/">PA</a>
          </li>
        
          <li>
            <a href="/2021/03/17/AlphaJoin%20%E8%BF%9E%E6%8E%A5%E9%A1%BA%E5%BA%8F%E9%80%89%E6%8B%A9%E5%99%A8/">AlphaJoin</a>
          </li>
        
          <li>
            <a href="/2021/03/17/%E3%80%8A%E8%AD%A6%E6%83%95%E6%80%9D%E7%BB%B4%E5%83%B5%E5%8C%96%E3%80%8B/">警惕思维僵化</a>
          </li>
        
          <li>
            <a href="/2021/03/17/Stable-Learned-Bloom-Filters-for-Data-Streams/">Stable Learned Bloom Filters for Data Streams</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2021 Bench Lian<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>