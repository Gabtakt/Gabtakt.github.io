<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Stable Learned Bloom Filters for Data Streams | Heyosam</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="Stable Learned Bloom Filters for Data StreamsAbstractBloom过滤器及其变体是用于近似集成员关系查询的数据结构，它具有优雅的、空间利用率高的特点。最近的研究表明，Bloom过滤器的空间成本可以通过与预先训练的机器学习模型的结合显著降低，命名为Learned Bloom Filters(LBF)。LBF通过使用分类器承担部分查询，减轻了Bloom">
<meta property="og:type" content="article">
<meta property="og:title" content="Stable Learned Bloom Filters for Data Streams">
<meta property="og:url" content="http://example.com/2021/03/17/Stable-Learned-Bloom-Filters-for-Data-Streams/index.html">
<meta property="og:site_name" content="Heyosam">
<meta property="og:description" content="Stable Learned Bloom Filters for Data StreamsAbstractBloom过滤器及其变体是用于近似集成员关系查询的数据结构，它具有优雅的、空间利用率高的特点。最近的研究表明，Bloom过滤器的空间成本可以通过与预先训练的机器学习模型的结合显著降低，命名为Learned Bloom Filters(LBF)。LBF通过使用分类器承担部分查询，减轻了Bloom">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/images/Stable-Learned-Bloom-Filters-for-Data-Streams/image-20201222165502920.png">
<meta property="og:image" content="http://example.com/images/Stable-Learned-Bloom-Filters-for-Data-Streams/image-20201222201107304.png">
<meta property="og:image" content="http://example.com/images/Stable-Learned-Bloom-Filters-for-Data-Streams/image-20201223145156086.png">
<meta property="og:image" content="http://example.com/images/Stable-Learned-Bloom-Filters-for-Data-Streams/image-20201223161635853.png">
<meta property="og:image" content="http://example.com/images/Stable-Learned-Bloom-Filters-for-Data-Streams/image-20201224103004372.png">
<meta property="og:image" content="http://example.com/images/Stable-Learned-Bloom-Filters-for-Data-Streams/image-20201224132615062.png">
<meta property="og:image" content="http://example.com/images/Stable-Learned-Bloom-Filters-for-Data-Streams/image-20201224165522871.png">
<meta property="og:image" content="http://example.com/images/Stable-Learned-Bloom-Filters-for-Data-Streams/image-20201229144707596.png">
<meta property="og:image" content="http://example.com/images/Stable-Learned-Bloom-Filters-for-Data-Streams/image-20201229145646750.png">
<meta property="og:image" content="http://example.com/images/Stable-Learned-Bloom-Filters-for-Data-Streams/image-20201229151825465.png">
<meta property="og:image" content="http://example.com/images/Stable-Learned-Bloom-Filters-for-Data-Streams/image-20201229170429692.png">
<meta property="og:image" content="http://example.com/images/Stable-Learned-Bloom-Filters-for-Data-Streams/image-20201229170443128.png">
<meta property="og:image" content="http://example.com/images/Stable-Learned-Bloom-Filters-for-Data-Streams/image-20201229170457285.png">
<meta property="article:published_time" content="2021-03-17T03:36:40.007Z">
<meta property="article:modified_time" content="2021-03-17T03:36:40.007Z">
<meta property="article:author" content="Bench Lian">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/Stable-Learned-Bloom-Filters-for-Data-Streams/image-20201222165502920.png">
  
    <link rel="alternate" href="/atom.xml" title="Heyosam" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Heyosam</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-Stable-Learned-Bloom-Filters-for-Data-Streams" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/03/17/Stable-Learned-Bloom-Filters-for-Data-Streams/" class="article-date">
  <time class="dt-published" datetime="2021-03-17T03:36:40.007Z" itemprop="datePublished">2021-03-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      Stable Learned Bloom Filters for Data Streams
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="Stable-Learned-Bloom-Filters-for-Data-Streams"><a href="#Stable-Learned-Bloom-Filters-for-Data-Streams" class="headerlink" title="Stable Learned Bloom Filters for Data Streams"></a>Stable Learned Bloom Filters for Data Streams</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>Bloom过滤器及其变体是用于近似集成员关系查询的数据结构，它具有优雅的、空间利用率高的特点。最近的研究表明，Bloom过滤器的空间成本可以通过与预先训练的机器学习模型的结合显著降低，命名为Learned Bloom Filters(LBF)。LBF通过使用分类器承担部分查询，减轻了Bloom过滤器的空间需求。然而，当前的LBF结构通常以静态成员集合为目标，当集合上有成员更新时，它们的性能将不可避免地下降，而这种更新需求在现实世界的数据流应用程序(如重复项检测、恶意URL检查和web缓存)中并不少见。为了使LBF适应数据流，我们提出了Stable Learned Bloom Filters (SLBF)，通过将分类器与可更新的备份过滤器相结合的方式来解决密集插入工作负载上的性能衰减问题。具体地说，我们提出了两种SLBF结构，单SLBF (s-SLBF)和分组SLBF (g-SLBF)。对这两种结构的理论分析表明，SLBF的期望假正例率(FPR)随着新构件的加入渐近地成为一个常数。在真实数据集上的大量实验表明，SLBF引入了类似级别的假反例率(FNR)，但与针对数据流优化的最先进的(非学习的)Bloom过滤器相比，它有更好的FPR/存储权衡。</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>Bloom过滤器是一个简单、空间利用率高的概率数据结构，用于解决成员查询问题，也就是说查询一个元素$x$是否在集合中<em>S</em>中。由于它的重要性，关于Bloom过滤器及其应用的优化和变体，特别是在数据库和网络领域，在过去几十年里已经进行了大量的研究。尽管Bloom过滤器在学术界和工业界都得到了很好的拓展和评价，但最近的一项提议”The Case for Learned Index structure”中提出，神经网络等机器学习模型可以与B-tree、hash table、Bloom filters等传统索引结构相结合，进一步提高空间利用率和查询效率。</p>
<p>Kraska 等人在他们的开创性工作中提出，成员关系查询可以看作是集合${(x_i,y_i=1)|x_i∈S} ∪{(x_i,y_i=0)|x_i∈N}$上分类问题的一个实例，其中<em>S</em>和<em>N</em>是成员和非成员的集合。它们首先使用一个分类器将元素分类为成员/非成员，然后，为了消除FNR，在集合$S_N = {x|x∈S,x\ is\ predicted\ as\ non-member}$上构建一个小的备用Bloom过滤器，以区分真成员和预测的非成员，这种数据结构即是LBF，如下图所示</p>
<p><img src="/images/Stable-Learned-Bloom-Filters-for-Data-Streams/image-20201222165502920.png" alt="图1"></p>
<p>对于查询处理，如果分类器和备用过滤器都将查询的元素确定为非成员，认定该元素为非成员。例如图中展示的这个例子，集合$S={x,y,z}$，<em>x</em>被分类器正确地预测为成员，因此不需要进一步处理。对于被错误分类的<em>y</em>和<em>z</em>，我们将它们插入标准的Bloom过滤器中，该过滤器包含1个16位的数组和3个哈希函数。当用元素<em>w</em>查询构造的LBF时，假设分类器判定<em>w</em>为非成员，<em>w</em>将在备份过滤器上进一步测试，最终得到非成员决策，即<em>w</em> $\notin$<em>S</em>。</p>
<p>类似于标准BF, LBF也有片面的错误即FPR，与非学习过滤器相比，LBF的优点是：在一个静态元素集合上，它占用更小的存储空间，同时也能具有较高的查询效率和较低的错误率，原因是LBF的空间耗费来自于存储分类器和备份过滤器，但是$S_N$(用于构建备份过滤器的集合)相对较小，存储一个分类器通常只需要较小的空间。</p>
<p>然而，与标准BF类似，LBF是为静态元素集<em>S</em>设计的，其总基数是预先知道的，因此，当有新的元素时由于备份过滤器的结构决定了备份过滤器的空间有限，插入过滤器后，FPR必然增大。与标准BF相比，LBF因插入元素而造成的性能衰减效应更为严重，因为备份过滤器通常较小。</p>
<p>下图说明了这个问题</p>
<p><img src="/images/Stable-Learned-Bloom-Filters-for-Data-Streams/image-20201222201107304.png" alt="图2"></p>
<p>在假设新插入的元素来自与<em>S</em>相同的分布的情况下，通过统计FPR与插入数量的关系发现，在所有四种设置下，新插入80K后，FPR趋于100%。此外，虽然较小的$F_n$可以减缓FPR的增长，但它引入一个更高的初始FPR。直观上，这是因为$F_p$和$F_n$通常相互矛盾，而且Fp是FPR的下界。</p>
<p>虽然LBF空间效率高，但它只适用于预定义的元素集合和仅查询的工作负载，这限制了它在现实世界中的应用，并促使我们设计新的可用于插入密集型工作负载的学习过滤器。但是，构建这样一个支持插入的LBF并不简单，现有的解决动态插入问题的工作，所有目标标准BF，由于存在额外的分类器，它们不能直接应用于LBF上下文。主要的挑战在于以下4个方面：</p>
<ol>
<li>ML模型通常是不确定的，因此，我们需要设计一个新的数学模型来分析学习过滤器在数据流上的性能。</li>
<li>不同于标准BF，我们希望控制FPR，但当滤波器应用于一个存储空间有限的无界数据流时，不可避免地会引入FPR。因此，我们需要仔细量化FPR，并在FPR、FNR和存储之间实现适当的权衡。</li>
<li>当处理静态元素集时，分类器的过拟合通常是好的，因为它改善了$F_p$和$F_n$，但这与流数据的情况相反，因为我们希望分类器能很好地概括将来的元素。</li>
<li>在动态插入上下文中，参数设置变得更加困难。原来在静态元素集合上的LBF，备份过滤器的参数例如哈希函数的数目，是根据元素集的大小、分类器的性能和用户提供的FPR阈值进行优化的，该阈值会随着新元素插入过滤器而改变。</li>
</ol>
<p>为了处理新元素的动态插入，本文设计了一种新的支持插入的LBF结构：Stable LBF(SLBF)。SLBF有以下特性：</p>
<ol>
<li>性能衰减效得到控制，即使大量插入，FPR也有一个非平凡的上界。</li>
<li>在相同的错误率下，总存储花费是有限的，比使用标准过滤器的存储花费更低。</li>
<li>成员查询与标准过滤器一样有效。</li>
</ol>
<p>下图展示了SLBF的特性。</p>
<p><img src="/images/Stable-Learned-Bloom-Filters-for-Data-Streams/image-20201223145156086.png" alt="图3"></p>
<p>左边是在相同存储花费下FPR v.s. #insertions，右边是在相同的FPR期望上界下的性能特征。当应用于动态插入时，SLBF具有较低的存储花费，并能解决性能衰退问题，SLBF可以用于许多真实世界的应用，如重复检测，IP流量监控和搜索引擎优化。据我们所知，这是第一个考虑在动态插入工作负载上优化LBF的工作，我们所作的工作总结如下：</p>
<ol>
<li>介绍了两个新的Bloom过滤器，Simple Stable learned Bloom Filter(s-SLBF)和分Grouping  Stable learned Bloom Filter(g-SLBF)，并实现了上述的三个特性。</li>
<li>我们对我们提出的数据结构在动态插入工作负载下的性能进行了详细的分析，并解释了其参数设置和分类器选择。</li>
<li>我们对真实数据进行了大量的实验研究，结果表明g-SLBF可以有效降低高达97%的存储花费。</li>
</ol>
<h2 id="2-PRELIMINARIES"><a href="#2-PRELIMINARIES" class="headerlink" title="2. PRELIMINARIES"></a>2. <strong>PRELIMINARIES</strong></h2><p>本节首先对标准BF和LBF的结构和分析结果进行概述。然后，我们讨论了无界数据流上BF的稳定性。为了便于快速参考，下图总结了今后使用的所有符号。</p>
<p><img src="/images/Stable-Learned-Bloom-Filters-for-Data-Streams/image-20201223161635853.png" alt="图4"></p>
<h3 id="2-1-Standard-Bloom-Filter"><a href="#2-1-Standard-Bloom-Filter" class="headerlink" title="2.1 Standard Bloom Filter"></a>2.1 Standard Bloom Filter</h3><p>给定一个包含n个元素的集合<em>S</em>，标准BF使用大小为$m$位数组$B$，独立的哈希函数$h_1，···，h_k$将元素映射到<em>1 ~ m</em>。数组<em>B</em>初始化为0，然后，对于每一个$x∈S$，位于$h_0(x)，···，h_k(x)$处的位均设为1。对于元素y的隶属性检验，如果$h_0(y)，····，h_k(y)$所指向的k位均为1，则为正样本，否则为负样本。这样的构造和查询机制确保不存在假负例，但可能存在假正例。在把<em>S</em>的所有n个元素插入位数组<em>B</em>后，<em>B</em>的任意位仍然为0的概率可根据以下公式计算：<br>$$<br>Pr(B[i]=0)=(1-\frac{1}{m})^{kn}\approx e^{-kn}/{m}\qquad\qquad(1)<br>$$<br>用$p0=Pr(B[i]=0)$表示，对于任意非成员元素$y \notin S$，则FPR为：<br>$$<br>FPR_{BF}=Pr(B[h_1(x)]=1\cap\cdots B[h_k(x)]=1)<br>=(1-p0)^k\approx(1-e^{-kn/m})^k\qquad\qquad(2)<br>$$<br>给定n和m，对上式求导，使其导数为0，得到哈希函数的最佳数目是$k^{opt}=\frac{m}{n}ln2$，对应的FPR的最小值是$0.5^{\frac{m}{n}ln2}\approx 0.6185\frac{m}{n}$。更一般地，标准BF的FPR可以建模为$\alpha^t$，其中$\alpha \in (0,1)$，$t=\frac{m}{n}$。</p>
<h3 id="2-2-Learned-Bloom-Filter"><a href="#2-2-Learned-Bloom-Filter" class="headerlink" title="2.2 Learned Bloom Filter"></a>2.2 Learned Bloom Filter</h3><p>LBF的正式定义由Kraska等人提出，Mitzenmacher进一步完善，定义如下：</p>
<p><em><strong>定义1：</strong></em>给定一个包含n个元素的集合$S$，LBF可以用一个三元组$(f,\tau,BF)$表示，其中$f:x \in S \rightarrow [0,1]$是一个预先训练好的分类器，$\tau$是决策阈值，当$f(x) \geq \tau$时，则决策$x \in S$，$BF$是建立在$S$中被错误预测为非成员的所有元素集合上的标准备份BF，即：${x|f(x)&lt;\tau,x\in S}$。</p>
<p>如图1所示，当处理元素y的成员关系查询时，我们相信正样本的预测结果，但质疑来自分类器的负样样本输出，当且仅当分类器和备份过滤器都确定y是非成员时，才认定是负样本。这样的LBF结构保证了单点误差(即没有FN，只有FP)，对于任意非成员元$y \notin S$, FPR为：<br>$$<br>FPR_{LBF} = Pr(f(y)\geq\tau) + Pr(f(y) &lt; \tau)\cdot\alpha^{m/|S_N|}\qquad\qquad(3)<br>$$<br>其中，$m$是备份过滤器的数组大小，$S_n={x|f(x)&lt;\tau,x\in S}$，$\alpha$是一个常数，取决于备份过滤器的实现。对上面这个公式，$Pr(f(y)\geq \tau)$可以解释成分类器的FP率，它本质上是一个随机变量，取决于y是怎么取的，即查询分布。在统计文献中，$Pr(f(y)\geq\tau)$可以通过使用探测数据集来估计，该探测数据集假设是从用于训练分类器$f$的数据集的相同分布中采样的。给定一个LBF ($f,\tau,BF$)，假设分类器$f$和备份过滤器BF分别使用$\zeta$位和$m$位，结合公式(2)和公式(3)，如果下列不等式成立，LBF比使用相同空间(即$\zeta+m$位)的标准BF好：<br>$$<br>F_p+(1-F_p)\cdot\alpha^{b/F_n}&lt;\alpha^{\zeta/n+b}\qquad\qquad(4)<br>$$<br>其中，$F_p$是分类器的FP率，$F_n=|S_N|/n,b=m/n,\alpha$均为常数。该不等式的左右两边分别表示LBF和BF的FPR，两者具有相同的空间和最优的哈希函数个数。</p>
<h3 id="2-3-BF-Stability-on-Data-Streams"><a href="#2-3-BF-Stability-on-Data-Streams" class="headerlink" title="2.3 BF Stability on Data Streams"></a>2.3 BF Stability on Data Streams</h3><p>无论构造标准BF还是LBF，都依赖于对元素集合$S$的全体的了解。为了开始我们关于动态增长元素集上BF稳定性的讨论，我们首先定义数据流上的隶属性测试。</p>
<p><em><strong>定义2：</strong></em>(数据流上的成员查询)考虑一个无约束流的元素$x_1,\cdots,x_n$,n可以无限，查询元素y， y的查询返回true当且仅当 $y\in {x_1,\cdots,x_n}$，即y在时间戳n之前被查询到。</p>
<p>如前所述，任何使用有限空间的BF都不能在无界数据流上实现有界单边错误。直观上，这可以用$FPR=\alpha^{m/n}$(在2.1节中的标准BF中定义)这个模型来解释，由模型可知：$lim_{n\rightarrow \infty}FPR=1$。为了使用有限的存储空间实现非平凡的FPR上界，Deng和Rafiei首先引入了SBF的概念，该概念提出了在插入元素时清除随机比特位的想法，以便为未来的元素腾出空间。</p>
<p>SBF表示动态增长的集合，使用一个大小为$m$的计数器数组$SBF[1,\cdots,m]$，而不是像标准BF那样使用bit数组，每个计数器分配$d$位bits，即$SBF[i]$的取值在$0$和$Max=2^d-1$之间。为了插入元素$x$，首先随机选择$P$个计数器，如果它们不为零，则减1。然后，与标准BF类似，得到$K$个独立的哈希函数值$h_1(x),\cdots,h_K(x)$，计数器$SBF[h_1(x)],\cdots,SBF[h_K(x)]$被设定为$Max$。对一个成员查询元素y，SBF返回true当$SBF[h_1(y)],\cdots,SBF[h_K(y)]$均不为0，否则返回false。使用SBF的插入算法和隶属性查询处理如图4a和图4b所示。</p>
<p><img src="/images/Stable-Learned-Bloom-Filters-for-Data-Streams/image-20201224103004372.png" alt="图4"></p>
<p>当将SBF应用于数据流$x_1,\cdots,x_n$时，一个关键的观察结果是，当插入数$n\rightarrow\infty$时，数组中计数器为0的数量趋于一个常数。给定一个有$m$个计数器的SBF，记为$p_0^{(n)}$为插入$n$个元素后值为0的计数器的比例，则$p_0^{(n)}$的极限为：<br>$$<br>\lim_{n \to \infty}(\frac{1}{1+\frac{1}{P(1/K-1/m)}})^{Max}\qquad\qquad (5)<br>$$<br>此外， $p_0^{(n)}-p_n^{(n-1)}\approx\frac Km(1-\frac Km)^n$，这说明$P_0^{(n)}$是指数收敛的。</p>
<p>$p_0^{(n)}$可以解释为在数据流中插入$n$个元素后任意计数器$SBF[i]$为0的概率。因此，对于不在数据流中的任意查询元素y，SBF的FPR为：<br>$$<br>\lim_{n\to\infty}FPR_{SBF}=\lim_{n\to\infty}(1-p_0^{(n)})^K\approx_{m\gg K}(1-(\frac1{1+K/P})^{Max})^K\qquad\qquad(6)<br>$$</p>
<p>对于数据流上的BF来说，这种在大量插入之后到达一个非平凡FPR而不是衰减到1的特性，被称为“稳定的”。</p>
<p>然而，每次随机选择计数器减1操作，导致SBF实现稳定的代价是FN非零，这意味着已经插入的元素$x_i$可能会被SBF错误地判定为非成员。研究表明SBF的FNR不仅与过滤器的参数有关，还与查询分布有关。关于数据流上FNR的细节讨论将在3.3节中介绍。</p>
<h3 id="2-4-Problem-Statement"><a href="#2-4-Problem-Statement" class="headerlink" title="2.4 Problem Statement"></a>2.4 Problem Statement</h3><p>我们已经概述了标准BF、LBF和SBF的结构和分析结果。在静态元素集合上，与已经使用了几十年的标准BF相比，LBF算法在降低内存开销方面具有优势。这启发我们设计一种新的LBF结果，用于动态元素集合上的近似成员查询(如定义2所示)。</p>
<p>具体地说，我们考虑这种数据结构的两种操作：</p>
<ol>
<li>将元素$x$插入到过滤器中。</li>
<li>使用过滤器返回元素y的成员关系测试结果的查询。</li>
</ol>
<p>当应用于数据流时，期望该LBF能达到稳定特性(即$n\rightarrow\infty$时FPR达到非平凡值)，以及与在数据流上优化的非学习过滤器(如SBF)相比在相同FPR/FNR级别上消耗更少的存储空间。</p>
<h2 id="STABLE-LEARNED-BLOOM-FILTER"><a href="#STABLE-LEARNED-BLOOM-FILTER" class="headerlink" title="STABLE LEARNED BLOOM FILTER"></a>STABLE LEARNED BLOOM FILTER</h2><p>在本节中，我们将介绍两种数据结构(第3.1节和第3.2节)以及理论分析(第3.3节)，以解决在学习索引背景下的流数据的近似隶属性测试问题。</p>
<h3 id="3-1-Single-SLBF"><a href="#3-1-Single-SLBF" class="headerlink" title="3.1 Single SLBF"></a>3.1 Single SLBF</h3><p>为了使原始的LBF框架在无限次插入后任然保持稳定，一个直观的想法是用一个稳定的BF(2.3节中介绍)取代LBF中的标准备份过滤器。如图5a所示，这种结构称为single stable Learned Bloom filter(s-SLBF)，其中single表示在该框架中有一个备份过滤器。</p>
<p><em><strong>定义3：</strong></em>一个s-SLBF可以表示成一个三元组$(f,\tau,SBF)$，其中$f$是一个预先训练好的分类器，$\tau$是相应的决策阈值，$SBF$是稳定备份过滤器。</p>
<p><img src="/images/Stable-Learned-Bloom-Filters-for-Data-Streams/image-20201224132615062.png" alt="图5"></p>
<p>插入一个新的元素$x$时(如图4c)，首先计算$f(x)$，并与阈值$\tau$比较，若$f(x)&gt;\tau$，则说明模型判断$x$已经预测为集合成员，插入过程立即终止；否则，$x$被插入$SBF$。查询元素y(如图4d)时，若$f(y)\geq \tau$或者$f(y)&lt;\tau$但$SBF$判断$y$是正样本，则返回正样本结果。</p>
<p>虽然与原始LBF非常相似，但s-SLBF中的分类器(即$f$和$\tau$)获得的方式本质上是不同的。回顾原始LBF分类器$f$的构造，是通过一个二分类数据集${x_i,y_i=1|x_y\in S}\cup{x_i,y_i=0|x_i\in N}$，其中$S$是构造过滤器的集合，它是静态的，$N$是由负样本组成的集合。相比之下，对于s-SLBF，由于它是在数据流进入之前建立的，而不是精确的元素集合(即$S$)，因此可以利用先验知识来训练分类器。这就产生了LBF和SLBF之间适用场景的根本区别(详细讨论在附录A)。</p>
<p>显然，如果我们假设数据流遵循训练分类器时使用的分布，s-SLBF在大量插入后是稳定的，根据公式(3)和公式(6)，设$SBF$的参数$m,K,P,Max$，则s-SLBF的FPR期望如下：<br>$$<br>E[FPR]=F_p+(1-F_p)\cdot(1-(\frac1{1+K/P})^{Max})^K\qquad\qquad(7)<br>$$<br>其中，$F_p=Pr_{y\sim D_N}(f(y)\geq\tau)$，$D_N$是非成员的分布。</p>
<p>如果分类器表现良好，在相同的预期FPR水平下，s-SLBF应该比原始SBF节省更多空间，我们使用下面的例子来解释这个优点。</p>
<p>根据公式(3)，对SBF，因为$m\gg K$，FPR的稳定对$m$不敏感。不失一般性，我们假定s-SLBF分类器的$F_p=0.01,F_n=0.5$，进一步选择SBF的参数$P,K<del>and</del>Max$满足$(1-(1/(1+K/P))^{Max})^K\approx0.1$。在这样的设定下，通过公式(7)，s-SLBF稳定时的FPR上界为$0.01 + 0.99∗0.1\approx0.1$，与使用相同参数的SBF相比处于同一水平。由于SBF的总存储成本是$m\cdot \lfloor log_2(Max) + 1\rfloor$ ，唯一影响总存储开销的因素是计数器的数量$m$($Max$是固定值)。由公式(5)和公式(6)，对SBF，$m$的增加或减少不影响稳定时的FPR，但会影响FPR收敛到稳定点的速度。因此，为了公平起见，我们比较了s-SLBF和SBF在相似稳定FPR和收敛速度下的存储开销差异。假设SBF和s-SLBF使用的计数器数分别为$m$和$m’$，让两个过滤器有相同的收敛速率，我们得到下面的方程：<br>$$<br>\frac Km(1-\frac Km)^N=\frac {K}{m’}(1-\frac{K}{m’})^{N\cdot F_n}\qquad\qquad(8)<br>$$<br>通过设定参数$m=10^6,K=6,N=10^8,F_n=0.5$，我们可以解得$m’\approx4.7\times10^4$，这表明存储开销降低了约53%。注意，我们忽略了s-SLBF中分类器造成的空间开销，因为它通常比计数器数组小得多。</p>
<p>为了进一步理解公式(8)中$m’$和$F_n$之间的关系，我们将$F_n$取值$0.3$到$0.5$,$N$取值为$5\times10^4$到$10^7$，结果如图6。随着$N$的增加相邻线之间的间隙减小，这表明当插入的数量$N$大大增加时，过滤器接近稳定点。我们可以观察到$m’$和$F_n$之间的近似线性关系，这是合理的，因为$F_n$决定有多少插入流中的元素从分类器“转义”并添加到备份SBF。分类器的$F_n$越高，意味着需要向备份SBF插入更多的元素，因此需要更多的计数器来保持相似的收敛速度。注意，上面的模拟修正了$F_p$等其他参数以简化分析，并提供对LBF优势的总体了解。</p>
<p><img src="/images/Stable-Learned-Bloom-Filters-for-Data-Streams/image-20201224165522871.png" alt="图6"></p>
<h3 id="3-2-Grouping-SLBF"><a href="#3-2-Grouping-SLBF" class="headerlink" title="3.2 Grouping SLBF"></a>3.2 Grouping SLBF</h3><p>作为一个直接扩展，s-SLBF已经被证明是稳定的，使用的存储空间可能比我们预期的SBF更低。然而，它也继承了原始LBF框架的一个主要缺点，即相信分类器做出的所有正样本预测，种过度依赖使得s-SLBF和原始LBF在分类器不可靠时变得脆弱。这种现象可以用公式(7)来解释，其中，分类器的$F_p$是整体FPR的下界。</p>
<p>除了对分类器的过度依赖问题，单个备份过滤器的设计方式也遗漏了分类器提供的有用信息。图5a说明了s-SLBF中分类器的工作方式，从中我们可以发现分类器的$F_p$和$F_n$都来自于决策阈值的设置，也就是说，位于左侧的负(正)样本都被归类为正(负)样本。这个生硬的决策规则不区分落在同一侧的元素的置信度，下面这个例子说明了这个问题：</p>
<p>假设决策阈值设为$\tau=0.7$，预测得分分别为0.69和0.01的元素没有差异，两者的处理方法相同，即反馈给同一个备份SBF。类似地，对于预测得分为0.71到0.99的元素，直接做出成员决策而没有反馈给备份SBF。</p>
<p>通过以上分析，我们认识到s-SLBF和原来的LBF的主要缺陷是单一备份过滤器的性质。为了进一步改进s-SLBF，我们引入了第二种数据结构：grouping stable Learned Bloom filter(g-SLBF)，将得分(范围[0,1])分类成几个区间，并为每个区间分配独立的子过滤器。</p>
<p><em><strong>定义4：</strong></em> g-SLBF由一个分类器$f$和$g$个不同的$SBF$(也称为子过滤器)$SBF_1,\cdots,SBF_g$构成，其中第$j$个$SBF$ $SBF_j$由参数$(m_j,K_j,P_j,Max_j)$描述。将区间$[0,1]$划分为$g$个子区间，即$[\tau_0 = 0,\tau_1],[\tau_1,\tau_2 ],\cdots,[\tau_{g-1},\tau_g=1]$，用于将元素$x$映射到$SBF$关于它们的预测值$f(x)$。更具体地说，如果$f(x)\in(\tau_{j-1}，\tau_j]$(如图4e所示)，则向$SBF_j$插入一个新元素$x$。为了测试元素y的成员隶属性，我们直接查询$f(y)$映射到的子过滤器$SBF_j$(如图4f所示)。</p>
<p>如图5b所示，g-SLBF的基本思想是根据分类器给出的隶属性置信度将插入流中的元素划分为几个组。直观地说，对于那些以低隶属性置信度插入的元素(即位于置信度分布的左侧，如图5b所示)，由于分类器在这个范围内$F_p$比较高，我们可以通过适当设置$K, P,Max$，调整相应子过滤器$SBF_j$来补偿FPR的损失。此外,因为$F_n$在这个范围内较低，这意味着不会有太多的元素要插入到$SBF_j$，需要分配的计数器较少，以在令人满意的收敛速度下稳定达到预期的FPR。另一方面，对于高置信度的元素(即位于置信度分布的右侧，如图5b所示)，SBF的FPR要求可以放宽，因为这个范围内的元素已经有很高的可能性是一个成员，同样的，因为分类器可能错误地确定许多成员元素分类为非成员(即高$F_n$)，因此需要更多的计数器让$SBF_j$收敛到稳定点。</p>
<p>值得注意的是，s-SLBF可以看作是g- SLBF的一种特殊情况，通过设置$g=2$，即只有一个决策阈值$\tau$，并且让子过滤器在范围$[\tau,1]$内做出正样本预测。与s-SLBF相比，g-SLBF的正预测是完全可信的，g-SLBF(g &gt; 2)对分类器输出更保守，因为所有的隶属性决策都是由分类器和备份过滤器共同做出的。这样的特性使得g-SLBF对分类器的质量更具有鲁棒性(例如，传入的元素流并不像训练数据那样严格遵循分布)。g-SLBF在鲁棒性方面的优势将在接下来的章节中通过分析和实验来展示。</p>
<h3 id="3-3-Analytical-Results"><a href="#3-3-Analytical-Results" class="headerlink" title="3.3 Analytical Results"></a>3.3 Analytical Results</h3><p>在本节中，我们分析了两种SLBF结构的FPR、FNR和收敛性。注意，我们之所以关注g-SLBF，是因为s-SLBF是g-SLBF的一种特殊情况，其理论结果自然适用于s-SLBF。我们首先在下面给出一些初步的记号。</p>
<p>对于g-SLBF中的第$j$个分类分数区间$[\tau_{j-1},\tau_j]$，我们定义两种概率$p_j$和$q_j$:<br>$$<br>p_j=Pr_{x\in D_N}(f(x)\in[\tau_{j-1},\tau_j]),\qquad\qquad \<br>q_j=Pr_{x\in D_P}(f(x)\in[\tau_{j-1},\tau_j])\qquad\qquad(9)<br>$$<br>其中，$D_N$和$D_P$是非成员和成员的分布。$(p_j,q_j)$描述了分类器在$[\tau_{j-1},\tau_j]$范围内的假阳性和假阴性行为。需要注意的是，在$D_N$和$D_P$未知的情况下，一般很难知道$p_j$和$q_j$的确切值。然而，如参照参数设置(第4节所示)，我们可以使用测试数据集来估计$p_j$和$q_j$。</p>
<p>在接下来的分析中，我们对分类器和数据采用了以下两个假设，这两个假设不难理解，并且已经被现有的LBF工作所采用。</p>
<p>**假设1.**过滤器的成员和非成员分别遵循分布$D_P$和$D_N$。因此，一个SLBF的FPR是在非成员分布$D_N$上的预期假阴性率(即在$D_N$上预测错误的预期概率)。</p>
<p><strong>假设2</strong>.对$j=1,\cdots,g$，有$p_1\ge p_2\ge \cdots \ge p_g$和$q_1\le q_2 \cdots \le q_g$。</p>
<h4 id="3-3-1-False-Positive-Rate-and-Stability"><a href="#3-3-1-False-Positive-Rate-and-Stability" class="headerlink" title="3.3.1 False Positive Rate and Stability"></a><em>3.3.1 False Positive Rate and Stability</em></h4><p>假设一个由n个元素$x_1,\cdots ,x_n$组成的序列被插入到g-SLBF中，其中$x_i\sim D_P$，对于来自非成员分布$D_N$中的元素的新查询，g-SLBF(稳定时)的期望FPR为：<br>$$<br>E[FPR]=\sum^{g}_{j=1}p_j\cdot(1-(\frac{1}{1+K_j/P_j})^{Max_j})^{K_j}\qquad\qquad(10)<br>$$<br>其中，$(1-(\frac{1}{1+K_j/P_j})^{Max_j})^{K_j}$可以表示为$\alpha_j$，假设有满足$\alpha_1\le \alpha_2  \le \cdots \le \alpha_g$的$g$个SBF。然后考虑<strong>假设2</strong>中描述的间隔，<strong>引理1</strong>描述了如何分配SBF的这些间隔来最小化$E[FPR]$，这也验证了我们在3.1节中的讨论。</p>
<p>**引理1.**分配过滤器中$\alpha_j$的间隔$[\tau_{j-1},\tau_j],j=1,\cdots,g$可以最小化$E[FPR]$。</p>
<p><em>证明：</em>根据假设有$p_1\ge p_2\ge \cdots \ge p_g$和$\alpha_1\le \alpha_2  \le \cdots \le \alpha_g$，根据重排不等式，对于任何其他排列$\alpha_{\sigma(1)},\alpha_{\sigma(2)}\cdots\alpha_{\sigma(g)}$，<br>$$<br>\sum^{g}<em>{j=1}p_j\cdot\alpha</em>{\sigma(j)}\ge\sum^{g}_{j=1}p_j\cdot\alpha_j=E[FPR]\qquad\qquad(11)<br>$$<br>在<strong>引理1</strong>的基础上，证明了g-SLBF(稳定)的期望FPR的一个上界，该上界不受$p_j$的约束。</p>
<p>**定理1(FPR上界定理).**g-SLBF在稳定状态下的期望FPR的上界是$g$个子过滤器$SBF_1,\cdots，SBF_g$的FPR的算术平均值，即$E[FPR] \le \frac {1}{g} \sum^{g}_{j=1} \alpha_j$。</p>
<p><em>证明：</em>因为$p_1\ge p_2\ge \cdots \ge p_g$和$\alpha_1\le \alpha_2  \le \cdots \le \alpha_g$，根据Chebyshev不等式和，下列式子总是成立的：<br>$$<br>E[FPR]=\sum^{g}<em>{j=1}p_j\cdot \alpha_j \le g \cdot (\frac{1}{g} \sum^{g}</em>{j=1}p_j)\cdot(\frac{1}{g} \sum^{g}<em>{j=1}\alpha_j)=1 \cdot \frac{1}{g} \sum^{g}</em>{j=1} \alpha_j=\frac{1}{g} \sum^{g}_{j=1} \alpha_j\qquad\qquad(12)<br>$$<br>回顾3.2节，我们认为g-SLBF中分类器质量的鲁棒性比s-SLBF强，正如我们之前讨论的， s-SLBF(以及原来的LBF)采用单一备份过滤器结构，从而使分类器的FPR直接上界于整体FPR。然而，通过<strong>定理1</strong>，g-SLBF的FPR以子过滤器的FPR的算术平均值的为界限，它独立于分类器的质量和特定的分布假设。注意，上述不等式的成立条件是假设$p_1\ge p_2\ge \cdots \ge p_g$，如果数据集是”可学习的”，这通常是成立的(参见我们在5.3节对这个假设的验证)。我们还进行了实验研究，通过增加元素流分布的畸变来验证鲁棒性，结果表明g-SLBF的FPR恶化速度远慢于s-SLBF，更多细节见附录E。</p>
<h4 id="3-3-2-Convergence-Rate"><a href="#3-3-2-Convergence-Rate" class="headerlink" title="3.3.2 Convergence Rate"></a><em>3.3.2 Convergence Rate</em></h4><p>下面的定理描述了g-SLBF的收敛速度，即过滤器接近稳定FPR的速度有多快。</p>
<p>**定理2(G-SLBF收敛定理).**g-SLBF以$O(exp(-C\cdot n))$的速率收敛于其稳定点，如公式(10)所示，其中$n$是总的插入数量，$C=min_j\frac{q_jm_j}{K_j},j=1,\cdots,g$。</p>
<p><em>证明：</em>如2.3节所介绍的，子过滤器$SBF_j$的收敛速度为：<br>$$<br>\frac{K_j}{m_j}(1-\frac{K_j}{m_j})^{q_j\cdot n}=\frac{K_j}{m_j}(1-\frac{K_j}{m_j})^{\frac{K_j}{m_j} \cdot \frac{m_jq_jn}{K_j}}\approx O(exp(-\frac{q_j}{k_j}n))\qquad\qquad(13)<br>$$<br>显然，当且仅当$SBF_1,\cdots,SBF_g$都是稳定的，g-SLBF才能达到稳定点，因此，总体收敛速度为最慢子过滤器的收敛速度，即$O(exp(-n\cdot min_j \frac{q_jm_j}{K_j}))$。</p>
<h4 id="3-3-3-False-Negative-Rate"><a href="#3-3-3-False-Negative-Rate" class="headerlink" title="3.3.3 False Negative Rate"></a><em>3.3.3 False Negative Rate</em></h4><p>当对成员的查询给出否定答案时，就会出现假阴性，即一个此前已经插入的元素被判断为非成员。于SBF类似，我们的g-SLBF允许大量的假阴性，以在无限制数据流上用有限的存储来实现有界的FPR(稳定)。为了量化假阴性对我们的数据结构的影响，我们首先回顾构成我们g-SLBF子的子过滤器SBF的FNR。</p>
<p>与仅由过滤器参数决定的FPR不同，SBF的FNR还依赖于输入数据流和查询工作量的特征。给定一个数据流中的元素$x_i$，设$\delta_i$为元素$x_i$最近一次查询和插入之间的次数，称为$x_i$的间隙。对于插入的元素$x_i$，元素$x_i$的假阴性概率为：<br>$$<br>Pr(FN_i)=1-\prod^K_{j=1}(1-Pr(SBF[h_j(x_i)]=0|\delta_i))\qquad\qquad(14)<br>$$<br>其中，$Pr(SBF[h_j(x_i)]=0|\delta_i)$表示计数器$SBF[h_j(x_i)]$在$\delta_i$次插入后变为0的概率。注意，如果$\delta_i&lt;Max$，那么$Pr(SBF[h_j(x_i)]=0|\delta_i)$总是0，因为计数器不可能减到0(回顾图4a中描述的插入SBF算法)。</p>
<p>对我们的g-SLBF，它采用相互独立的SBF序列做为子过滤器，假设在第$j$个子过滤器$SBF_j$中插入一个元素$x_i$，根据公式(14)，其假阴性概率为：<br>$$<br>Pr(FN_i|x_i\in SBF_j)=1-(1-p_N(\delta_i,k_{ij}))^{K_j}\qquad\qquad(15)<br>$$<br>其中，$p_N(\cdot)$表示$K_j$个过滤器($x_i$所映射)其中一个在$\delta_i$次插入操作已经减到0的概率($\delta_i是x_i的间隙$)。$p_N(\cdot)$是$\delta_i$和$k_{ij}$的函数，计算计数器被设置为$Max_j$的概率是多少。注意，$k_{ij}$是一个随机变量，它取决于数据流中每个元素的出现频率，也就是说，每个$x_i$的$p_N(\cdot)$都是不同的，这使得我们很难精确计算FNR,因为我们对这些插入频率没有先验知识。另一方面，在数据流足够大的情况下($n\to\infty$)，这种频率特征对整体的FNR结果影响不大。因此，在我们的工作中，在不失一般性的情况下，我们假定一个元素在插入数据流中只出现一次，即不存在重复插入，从而得到g-SLBF的期望FNR。</p>
<p>在上述假设下，每个$x_i$对应的$k_{ij}$都是相同的形式，即$k_{ij}=k_j=\frac{1}{nj}(1+\sum^{n_j-1}<em>{l=1}I_l)$，其中$n_j$是已经插入到过滤器$SBF_j$和的元素的数量，$I_l$是一个伯努利分布随机变量，满足$Pr(I_l=1)=K_j/m_j$。因此，g-SLBF的总体期望FNR可以被推导为：<br>$$<br>E[FNR]=\sum^g</em>{j=1}Pr(FN|x\in SBF_j)\cdot Pr(x\in SBF_j)=\sum^g_{j=1}(1-(1-p_N(\tilde\delta,k_j))^{K_j})\cdot q_j\qquad\qquad(16)<br>$$<br>其中，$\tilde\delta$是数据流的平均间隙。$p_N(\cdot)$基于$\tilde\delta$和$k_j$的具体评价请参见附录C。一旦$p_N$和过滤器参数确定，我们就可以利用上式估计FNR。</p>
<p>综上所述，在我们的g-SLBF上，作为在无限次插入后任然具有稳定性的一个副作用，假阴性是不可避免的，这与SBF相似。与FPR不同的是，g-SLBF的FNR的确定依赖于插入元素流和查询元素流的先验知识(计算每个插入元素$x_i$的间隙值$\delta_i$)。为了解决假阴性问题，在接下来的部分中，我们设计了一个参数设置策略，目标是最小化FNR，同时将FPR限定在一个用户给定的阈值内。第5节给出的详细评价结果表明，我们的g-SLBF与SBF相比有相似的FNR，但达到更好的FPR/存储比，即在相同的FNR和FPR水平下，我们所提出的学习过滤器可以节省更多的存储空间。</p>
<h4 id="3-3-4-Time-Complexity"><a href="#3-3-4-Time-Complexity" class="headerlink" title="3.3.4 Time Complexity"></a><em>3.3.4 Time Complexity</em></h4><p>使用g-SLBF进行插入操作和成员查询处理都需要$O(1)$时间。假设模型预测时间为$O(M)$，则插入和查询处理分别是$O(M+max_j(P_j+K_j))$和$O(M+max_j(K_j))$，因为所有的$M,P_j,K_j$都是指定的常量，我们的结论是，使用g-SLBF进行插入和隶属性测试需要常数时间。</p>
<h2 id="4-PARAMETER-SETTING"><a href="#4-PARAMETER-SETTING" class="headerlink" title="4. PARAMETER SETTING"></a>4. PARAMETER SETTING</h2><p>在这一节，我们将讨论如何根据分析的结果来正确设置g-SLBF的参数。同样，在不失一般性的情况下，我们将重点放在g-SLBF上，并且参数设置策略可以自然地扩展到s-SLBF。</p>
<p>之后再翻译，现在看这个意义不大</p>
<h2 id="5-EXPERIMENTAL-STUDY"><a href="#5-EXPERIMENTAL-STUDY" class="headerlink" title="5. EXPERIMENTAL STUDY"></a>5. EXPERIMENTAL STUDY</h2><p>在本节中，我们将报告在真实世界应用程序的数据集上的实现细节和实验结果。所有的实验都是在Intel(R) Core(TM) i7-8550U <a href="mailto:&#x43;&#80;&#x55;&#64;&#x31;&#x2e;&#x39;&#x39;&#x47;&#x48;&#122;">&#x43;&#80;&#x55;&#64;&#x31;&#x2e;&#x39;&#x39;&#x47;&#x48;&#122;</a>，内存16GB的Ubuntu笔记本电脑上进行的，所有的方法都使用C语言实现，使用GCC编译并使用O3参数优化。</p>
<h3 id="5-1-Baselines-and-Implementation-Details"><a href="#5-1-Baselines-and-Implementation-Details" class="headerlink" title="5.1 Baselines and Implementation Details"></a>5.1 Baselines and Implementation Details</h3><p>为了展示我们提供的数据结构的有效性，我们实现并比较了5个过滤器，包括标准BF(BF)，稳定BF(SBF)，传统学习BF(LBF)，简单SLBF(s-SLBF)和分组SLBF(g-SLBF)。</p>
<p><strong>BF和LBF.</strong> BF和LBF是本实验的基线，以展示非稳定过滤器在流数据上的表现。BF的实现遵循最标准的空间优化BF，其中哈希函数的数量总是设置为最优。对LBF，我们使用相对简单的模型(如梯度提升树)来考虑流数据场景中的效率，而不是使用的深度学习模型(细节将在后面讨论)。</p>
<p><strong>SBF,s-SLBF and g-SLBF.</strong> 这三种过滤器在流数据场景下实现了稳定性。SBF的参数和s-SLBF、g-SLBF的参数分别根据参考文献[13]和我们在第4节的讨论设置。</p>
<p><strong>Hash function Implementation.</strong> 所有过滤器都需要计算$K$个哈希值。我们采用xxHash，这是一种极快的高质量非加密的哈希方案。此外，我们不完全计算$K$个独立的哈希值，而是，只计算两个独立的哈希值$h_a,h_b$，并且第$j$个哈希值由$h_a+j*h_b,(j = 1,\cdots,K)$给出。</p>
<p><strong>Classifier Implementation.</strong> 在第一篇学习索引文献[25]中，建议使用深度学习模型，即神经网络模型来构建学习数据结构。具体来说，他们建议使用循环神经网络。尽管深度学习模型在很多任务中都有较好的表现，但是考虑到数据流上的成员查询处理对实施时间的要求，常见的深度学习框架如Tensorflow合pyTorch都过于笨重，难以部署。虽然使用GPU可以缓解推理效率的问题，但在CPU和GPU之间迁移数据可能成为一个新的瓶颈。由于这不是一篇介绍新的机器学习方案的论文，为了追求效率，我们测试并比较了三个轻量级模型：逻辑回归，支持向量机合基于Catboost的梯度提升树(GBT)。我们发现考虑分类器质量，存储开销和推理效率这三个因素GBT分类器表现得足够好，因此我们采用GBT作为LBF和SLBF的分类器。</p>
<h3 id="5-2-Datasets-Parameters-and-Metrics"><a href="#5-2-Datasets-Parameters-and-Metrics" class="headerlink" title="5.2 Datasets,Parameters and Metrics"></a>5.2 Datasets,Parameters and Metrics</h3><p>为了展示我们的SLBF的有效性与效率，我们测试了5种过滤器在三个数据集：Amazon,Attack,Higgs上的工作情况。下面我们简要介绍每个数据集，表2总结了这些数据集的统计信息。</p>
<p><strong>Task 1: Amazon.</strong> 该数据集包含从2010年到2011年收集的亚马逊员工的资源访问记录，在这些记录中，员工在一段时间内被允许或拒绝访问资源。每个记录包含一个唯一的ID和用于构建分类器的10个特征。</p>
<p><strong>Task 2: Attack.</strong> 这是一个网络攻击跟踪数据集。包括报文的源/目的和流量统计，总共提取了23个特征。</p>
<p><strong>Task 3: Higgs.</strong> 这是一个科学数据集，它要求对信号处理过程是否产生希格斯玻色子进行分类。通过粒子检测器得到的运动特征共有28个。通过粒子检测器得到的运动特征共有28个。</p>
<p><img src="/images/Stable-Learned-Bloom-Filters-for-Data-Streams/image-20201229144707596.png" alt="表2"></p>
<p>对每个数据集，采样其中一部分，具体来说是20%，用于估计预训练的分类器以及计算一些参数如$p_j,q_j$(公式(9))，剩下80%的数据用于生成插入和查询流。我们使用梯度提升树模型训练每个任务的分类器，模型信息如表3所示。</p>
<p><img src="/images/Stable-Learned-Bloom-Filters-for-Data-Streams/image-20201229145646750.png" alt="表3"></p>
<p><strong>Insertion Workloads.</strong> 将数据集中所有的正样本作为成员，负样本作为非成员，对一个数据集，相应的插入工作负载是正样本序列。注意，当向非学习的过滤器如BF和SBF插入元素时，我们只将其标识符插入过滤器，元素的特征信息将被丢弃。类似地，对学习过滤器如LBF，s-SLBF和g-SLBF，特征信息将被分类器用于计算隶属性得分，如果有必要，只向过滤器插入标识符。</p>
<p><strong>Query Workloads.</strong> 我们需要查询工作负载来评估所有过滤器的性能。通过第3节的分析，FPR和FNR分别由成员和非成员所测出。此外，FNR还受到插入到过滤器中的成员元素与被查询之间的时间间隔的影响。因此，对每个数据集，给定一个间隙值$\delta$，对于每个插入过滤器的元素$x_i$，我们将在其他元素插入$\delta$次后查询它来测量FNR。插入所有元素之后(来自每个task的正样本集)，我们将使用负样本集查询过滤器来度量FPR。然后将查询工作负载下的经验FNR和FPR计算为$EFNR=\frac { false\ negatives}{positive\ samples}$以及$EFPR=\frac {false\ positives}{negative\ samples}$。注意，在本节报告的结果中，我们设定$\delta$为2,000，这对于实际应用来说是一个合理的值。然而，我们也报告了通过改变$\delta$的结果，结果表明随着$\delta$的增加，对于SBF和我们的SLBF，FNR有明显的增加趋势。这是合理的，因为较高的$\delta$会增加后备过滤器中计数器下降到0的可能性，从而导致假阴性，更多信息参见附录D。</p>
<p><strong>Control variables.</strong> 用3个参数：FPR的上限$\epsilon$，总存储预算$B$，以及g-SLBF的组数$g$来评估过滤器的鲁棒性。表4总结了每个数据集的参数设置，其中下划线值被视为默认值。</p>
<p><img src="/images/Stable-Learned-Bloom-Filters-for-Data-Streams/image-20201229151825465.png" alt="表4"></p>
<h3 id="5-3-Experimental-Results"><a href="#5-3-Experimental-Results" class="headerlink" title="5.3 Experimental Results"></a>5.3 Experimental Results</h3><p><strong>Validation of Assumptions.</strong> 在3.3节分析我们的SLBF的性能时，我们假设一个训练好的分类器满足$p_1\ge p_2\ge \cdots \ge p_g$和$q_1\le q_2 \cdots \le q_g$，为了验证这一假设，我们使用Attack和Higgs的正样本集和负样本集来计算相应的分类得分，并绘制如图10所示的直方图，由此可以验证我们的假设。此外，分数直方图也可以用来指导$g$的设置，因为我们可以一直对区间进行划分，直到这种单调关系不成立为止。</p>
<p><strong>Validation of  Stability.</strong> 为了验证过滤器的稳定性，我们用Amazon数据集测试g-SLBF($g=6,\epsilon=10%,B\ in\ range\ 2^{10}\cdots 2^{18}$)，具体来说，我们在每4000次新插入后使用负样本集来测量经验FPR，并绘制图11中的结果。我们发现EFPR随着新元素的插入增长，最后达到稳定值0.1，此外，$B$越小，过滤器的收敛速度越快。g-SLBF在其他数据集和s-SLBF上的稳定性结果与图11相似。</p>
<p><img src="/images/Stable-Learned-Bloom-Filters-for-Data-Streams/image-20201229170429692.png" alt="图8"></p>
<p><img src="/images/Stable-Learned-Bloom-Filters-for-Data-Streams/image-20201229170443128.png" alt="图9"></p>
<p><img src="/images/Stable-Learned-Bloom-Filters-for-Data-Streams/image-20201229170457285.png" alt="图10"></p>
<p><strong>Overall Comparison.</strong> 本次实验在3个数据集上测试了5种过滤器，通过修改$g$和$\epsilon$的默认值，经验FPR和FNR(使用查询工作负载计算)与比特预算B的比较如图8所示。注意，$B$是指分配给(备份)过滤器，这也是BF和SBF的实际存储成本。对LBF，g-SLBF和s-SLBF，它们的实际存储成本是$B$加上分类器模型的大小。然而，如表3所示，我们可以发现模型大小是恒定的，并且相对较小。更具体地，考虑使BF和LBF产生图8a-8c中的显著FPR的最小$B$，例如，$B\approx2^{20}\ for\ Amazon,B\approx2^{22}\ for\ Attack\ and\ B\approx2^{25}\ for \ Higgs$(，)这意味着当$B$小于这个值时，BF或LBF的FPR都太高且无法忍受)，即使在这种情况下，分类器的成本也只占$B$的0.5% ~ 16%。因此，对于所有的5个过滤器，我们可以简单地把比特预算$B$作为它们的存储成本。</p>
<p>我们首先比较5种过滤器在不同$B$的情况下的FPR，如图8a-8c所示。当B非常小时，BF或LBF的FPR都非常高(接近100%)，但会随着B的增加而下降，这意味着它们需要分配一个相当大的$B$来达到可观的FPR。相比之下，SBF，s-SLBF和g-SLBF的FPR都在5%以内，这是我们学习过滤器的默认设置，即默认FPR的上界。我们可以观察到，在相同存储成本下，我们的g-SLBF的FPR总是优于其他过滤器，相比相同FPR下的存储成本，其优势更加明显。例如，在Attack数据集上，$B=2^{18}$时g-SLBF的FPR和$B = 2^{24}$时SBF的FPR约为0.1%，但是它们的存储成本比为$\frac{2^{18}Bit+173KBits}{2^{24}Bit}\times100%\approx2.6%$，这意味着g-SLBF需要的存储空间仅为SBF的2.6%。对于FNR，如图8-8f所示，随着$B$的增加，SBF，s-SLBF和g-SLBF的表现相似。我们的学习SBF在FNR上的性能似乎没有明显优于SBF。原因是，一个好的分类器可以通过提供一个好的预测分数来降低FPR，但是它对避免错误否定没有太大帮助，因为否定(非成员)决策仅由备份过滤器做出。</p>
<p>此外，我们发现g-SLBF的FPR可以通过分配更多的比特位来不断降低，而s-SLBF则不适用，因为分类器的FPR是s-SLBF整体FPR的下界。这验证了我们的论点，即g-SLBF对分类器输出更保守，因此更稳健。</p>
<p><strong>Varying  $\epsilon$.</strong> 在本实验中，我们研究了FPR上限，参数ε对g-SLBF的影响。我们将g-SLBF的FNR w.r.t. ε绘制在图5-4(a)中，可以清楚地看出ε与FNR呈反比关系，即期望FPR越高，假阴性的数量就越少。这是合理的，因为根据公式(3-6)，较高的ε意味着较低的P和较高的Max，这使得计数器更难减少到0，从而降低FNR。</p>
<p>​    </p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2021/03/17/Stable-Learned-Bloom-Filters-for-Data-Streams/" data-id="ckmcwaa630003rksq9k233wx9" data-title="Stable Learned Bloom Filters for Data Streams" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2021/03/17/%E3%80%8A%E8%AD%A6%E6%83%95%E6%80%9D%E7%BB%B4%E5%83%B5%E5%8C%96%E3%80%8B/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          警惕思维僵化
        
      </div>
    </a>
  
  
    <a href="/2021/03/17/hello-world/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Hello World</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/03/">March 2021</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2021/03/17/PAlab-report/">PA</a>
          </li>
        
          <li>
            <a href="/2021/03/17/AlphaJoin%20%E8%BF%9E%E6%8E%A5%E9%A1%BA%E5%BA%8F%E9%80%89%E6%8B%A9%E5%99%A8/">AlphaJoin</a>
          </li>
        
          <li>
            <a href="/2021/03/17/%E3%80%8A%E8%AD%A6%E6%83%95%E6%80%9D%E7%BB%B4%E5%83%B5%E5%8C%96%E3%80%8B/">警惕思维僵化</a>
          </li>
        
          <li>
            <a href="/2021/03/17/Stable-Learned-Bloom-Filters-for-Data-Streams/">Stable Learned Bloom Filters for Data Streams</a>
          </li>
        
          <li>
            <a href="/2021/03/17/hello-world/">Hello World</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2021 Bench Lian<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>